import json
import os
from typing import Dict, Any, List
import requests
from collections import defaultdict
import math
import pymorphy3

morph = pymorphy3.MorphAnalyzer()

def lemmatize_phrase(phrase: str) -> str:
    '''–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ñ—Ä–∞–∑—ã ‚Äî –ø—Ä–∏–≤–æ–¥–∏—Ç —Å–ª–æ–≤–∞ –∫ –Ω–∞—á–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ'''
    words = phrase.lower().split()
    lemmas = []
    for word in words:
        parsed = morph.parse(word)[0]
        lemmas.append(parsed.normal_form)
    return ' '.join(lemmas)

def detect_intent(phrase: str) -> str:
    '''–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π –∏–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π intent'''
    commercial_markers = [
        '–∫—É–ø–∏—Ç—å', '—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–∑–∞–∫–∞–∑–∞—Ç—å', '–¥–æ—Å—Ç–∞–≤–∫–∞', 
        '–Ω–µ–¥–æ—Ä–æ–≥–æ', '–¥–µ—à–µ–≤–æ', '–º–∞–≥–∞–∑–∏–Ω', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç', '—Å–∞–π—Ç',
        '–ø—Ä–æ–¥–∞–∂–∞', '—Å–∫–∏–¥–∫–∞', '–∞–∫—Ü–∏—è', '–æ—Ñ–∏—Å', '—Ç–µ–ª–µ—Ñ–æ–Ω'
    ]
    
    info_markers = [
        '–∫–∞–∫', '—á—Ç–æ', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∫–∞–∫–æ–π',
        '–æ—Ç–∑—ã–≤—ã', '—Ä–µ–π—Ç–∏–Ω–≥', '–ª—É—á—à–∏–π', '—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ', '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è',
        '—Ñ–æ—Ç–æ', '–≤–∏–¥–µ–æ', '—Å—Ç–∞—Ç—å—è', '—Ñ–æ—Ä—É–º'
    ]
    
    phrase_lower = phrase.lower()
    
    commercial_score = sum(1 for marker in commercial_markers if marker in phrase_lower)
    info_score = sum(1 for marker in info_markers if marker in phrase_lower)
    
    if commercial_score > info_score:
        return 'commercial'
    elif info_score > commercial_score:
        return 'informational'
    else:
        return 'general'

def detect_minus_words(phrases: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
    '''
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–∏–Ω—É—Å-—Å–ª–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π —Ä–µ–∫–ª–∞–º—ã
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –Ω–µ—Ü–µ–ª–µ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    '''
    minus_categories = {
        'free': {
            'name': 'üÜì –ë–µ—Å–ø–ª–∞—Ç–Ω–æ / –•–∞–ª—è–≤–∞',
            'keywords': ['–±–µ—Å–ø–ª–∞—Ç–Ω–æ', '–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π', '–¥–∞—Ä–æ–º', '–±–µ–∑–≤–æ–∑–º–µ–∑–¥–Ω–æ', '–∑–∞–¥–∞—Ä–º–∞', 'free'],
            'phrases': []
        },
        'diy': {
            'name': 'üîß –°–≤–æ–∏–º–∏ —Ä—É–∫–∞–º–∏ / DIY',
            'keywords': ['—Å–≤–æ–∏–º–∏ —Ä—É–∫–∞–º–∏', '—Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ', '—Å–∞–º', '—Å–∞–º–æ–º—É', 'diy', '–∫–∞–∫ —Å–¥–µ–ª–∞—Ç—å', '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è'],
            'phrases': []
        },
        'competitors': {
            'name': 'üè¢ –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã / –ü–ª–æ—â–∞–¥–∫–∏',
            'keywords': ['–∞–≤–∏—Ç–æ', '—Ü–∏–∞–Ω', '–¥–æ–º–∫–ª–∏–∫', '—è–Ω–¥–µ–∫—Å –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å', '—é–ª–∞', '–∏–∑ —Ä—É–∫ –≤ —Ä—É–∫–∏'],
            'phrases': []
        },
        'info': {
            'name': '‚ÑπÔ∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã',
            'keywords': ['—á—Ç–æ —Ç–∞–∫–æ–µ', '–∫–∞–∫ –≤—ã–±—Ä–∞—Ç—å', '–∫–∞–∫–æ–π –ª—É—á—à–µ', '–æ—Ç–ª–∏—á–∏—è', '—Ä–∞–∑–Ω–∏—Ü–∞', '–ø–ª—é—Å—ã –º–∏–Ω—É—Å—ã', '—Å–æ–≤–µ—Ç—ã'],
            'phrases': []
        },
        'job': {
            'name': 'üíº –†–∞–±–æ—Ç–∞ / –í–∞–∫–∞–Ω—Å–∏–∏',
            'keywords': ['–≤–∞–∫–∞–Ω—Å–∏–∏', '—Ä–∞–±–æ—Ç–∞', '—Ä–µ–∑—é–º–µ', '–∑–∞—Ä–ø–ª–∞—Ç–∞', '—Ç—Ä–µ–±—É—é—Ç—Å—è', '–∏—â—É —Ä–∞–±–æ—Ç—É', '–∫–∞—Ä—å–µ—Ä–∞'],
            'phrases': []
        },
        'education': {
            'name': 'üéì –û–±—É—á–µ–Ω–∏–µ / –ö—É—Ä—Å—ã',
            'keywords': ['–∫—É—Ä—Å—ã', '–æ–±—É—á–µ–Ω–∏–µ', '—Å–µ–º–∏–Ω–∞—Ä', '—Ç—Ä–µ–Ω–∏–Ω–≥', '–≤–µ–±–∏–Ω–∞—Ä', '–º–∞—Å—Ç–µ—Ä –∫–ª–∞—Å—Å', '—É—Ä–æ–∫–∏'],
            'phrases': []
        },
        'download': {
            'name': 'üì• –°–∫–∞—á–∞—Ç—å / –ó–∞–≥—Ä—É–∑–∏—Ç—å',
            'keywords': ['—Å–∫–∞—á–∞—Ç—å', '–∑–∞–≥—Ä—É–∑–∏—Ç—å', 'download', '—Ç–æ—Ä—Ä–µ–Ω—Ç', '–æ–Ω–ª–∞–π–Ω', '—Å–º–æ—Ç—Ä–µ—Ç—å'],
            'phrases': []
        },
        'porn': {
            'name': 'üîû –í–∑—Ä–æ—Å–ª—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç',
            'keywords': ['–ø–æ—Ä–Ω–æ', '—Å–µ–∫—Å', 'xxx', '—ç—Ä–æ—Ç–∏–∫–∞', '–∏–Ω—Ç–∏–º'],
            'phrases': []
        },
        'other': {
            'name': '‚ùì –ü—Ä–æ—á–∏–µ –Ω–µ—Ü–µ–ª–µ–≤—ã–µ',
            'keywords': ['–∏–≥—Ä–∞', '–∏–≥—Ä—ã', '–º—É–ª—å—Ç—Ñ–∏–ª—å–º', '–∫–∞—Ä—Ç–∏–Ω–∫–∏', '—Ä–∏—Å—É–Ω–æ–∫', '—Ä–∞—Å–∫—Ä–∞—Å–∫–∞', '—à—É—Ç–∫–∏', '–∞–Ω–µ–∫–¥–æ—Ç—ã'],
            'phrases': []
        }
    }
    
    for phrase_data in phrases:
        phrase_lower = phrase_data['phrase'].lower()
        
        matched = False
        for category_key, category_data in minus_categories.items():
            for keyword in category_data['keywords']:
                if keyword in phrase_lower:
                    category_data['phrases'].append(phrase_data)
                    matched = True
                    break
            if matched:
                break
    
    result = {}
    for key, data in minus_categories.items():
        if len(data['phrases']) > 0:
            result[key] = {
                'name': data['name'],
                'count': len(data['phrases']),
                'total_volume': sum(p['count'] for p in data['phrases']),
                'phrases': sorted(data['phrases'], key=lambda x: x['count'], reverse=True)
            }
    
    return result

def calculate_tfidf(phrases: List[str]) -> List[Dict[str, float]]:
    '''–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è TF-IDF –±–µ–∑ scikit-learn'''
    stop_words = {
        '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–∏–∑', '–∏', '–∏–ª–∏', '–∫–∞–∫', '—á—Ç–æ', '–∑–∞',
        '—ç—Ç–æ', '—Ç–æ', '—Ç–∞–∫', '–Ω–æ', '–∞', '–æ', '—É', '–æ—Ç', '–∫', '–¥–æ', '–ø—Ä–∏',
        '–±–µ–∑', '–ø–æ–¥', '–Ω–∞–¥', '–º–µ–∂–¥—É', '–ø–µ—Ä–µ–¥', '—á–µ—Ä–µ–∑', '–ø–æ—Å–ª–µ'
    }
    
    doc_words = []
    for phrase in phrases:
        words = [w for w in phrase.lower().split() if w not in stop_words and len(w) > 2]
        doc_words.append(words)
    
    word_doc_count = defaultdict(int)
    for words in doc_words:
        unique_words = set(words)
        for word in unique_words:
            word_doc_count[word] += 1
    
    n_docs = len(phrases)
    idf = {}
    for word, count in word_doc_count.items():
        idf[word] = math.log(n_docs / count)
    
    tfidf_vectors = []
    for words in doc_words:
        word_count = defaultdict(int)
        for word in words:
            word_count[word] += 1
        
        tf = {w: count / len(words) if len(words) > 0 else 0 for w, count in word_count.items()}
        
        tfidf = {w: tf[w] * idf.get(w, 0) for w in tf}
        tfidf_vectors.append(tfidf)
    
    return tfidf_vectors

def cosine_similarity_simple(vec1: Dict[str, float], vec2: Dict[str, float]) -> float:
    '''–ö–æ—Å–∏–Ω—É—Å–Ω–∞—è –±–ª–∏–∑–æ—Å—Ç—å –º–µ–∂–¥—É –¥–≤—É–º—è –≤–µ–∫—Ç–æ—Ä–∞–º–∏'''
    all_words = set(vec1.keys()) | set(vec2.keys())
    
    dot_product = sum(vec1.get(w, 0) * vec2.get(w, 0) for w in all_words)
    
    norm1 = math.sqrt(sum(v**2 for v in vec1.values()))
    norm2 = math.sqrt(sum(v**2 for v in vec2.values()))
    
    if norm1 == 0 or norm2 == 0:
        return 0.0
    
    return dot_product / (norm1 * norm2)

def clusterize_with_openai(phrases: List[Dict[str, Any]], mode: str = 'context') -> List[Dict[str, Any]]:
    '''
    –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ OpenAI GPT-4o-mini
    '''
    openai_key = os.environ.get('OPENAI_API_KEY')
    if not openai_key:
        print('[OPENAI] API key not found, falling back to TF-IDF')
        return smart_clusterize(phrases, mode)
    
    print(f'[OPENAI] API key found: {openai_key[:10]}...')
    
    phrases_text = '\n'.join([f"{p['phrase']} ({p['count']} –ø–æ–∫–∞–∑–æ–≤)" for p in phrases[:200]])
    
    prompt = f"""–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è {'–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π —Ä–µ–∫–ª–∞–º—ã' if mode == 'context' else 'SEO'}.

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ñ—Ä–∞–∑—ã –∏ —Ä–∞–∑–¥–µ–ª–∏ –∏—Ö –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä—ã –ø–æ —Å–º—ã—Å–ª—É –∏ –∏–Ω—Ç–µ–Ω—Ç—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–ü—Ä–∞–≤–∏–ª–∞:
- {'–£–∑–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã (5-15 —Ñ—Ä–∞–∑) –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Ç–∞—Ä–≥–µ—Ç–∏–Ω–≥–∞' if mode == 'context' else '–®–∏—Ä–æ–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã (10-30 —Ñ—Ä–∞–∑) –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞'}
- –ù–∞–∑–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ –æ—Ç—Ä–∞–∂–∞–µ—Ç –≥–ª–∞–≤–Ω—ã–π –∏–Ω—Ç–µ–Ω—Ç
- –ö–∞–∂–¥–∞—è —Ñ—Ä–∞–∑–∞ —Ç–æ–ª—å–∫–æ –≤ –æ–¥–Ω–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ

–§—Ä–∞–∑—ã:
{phrases_text}

–í–µ—Ä–Ω–∏ JSON:
{{
  "clusters": [
    {{
      "cluster_name": "–ù–∞–∑–≤–∞–Ω–∏–µ",
      "intent": "commercial/informational/navigational",
      "phrases": [{{"phrase": "—Ç–µ–∫—Å—Ç", "count": —á–∏—Å–ª–æ}}]
    }}
  ]
}}"""
    
    try:
        response = requests.post(
            'https://api.openai.com/v1/chat/completions',
            headers={
                'Authorization': f'Bearer {openai_key}',
                'Content-Type': 'application/json'
            },
            json={
                'model': 'gpt-4o-mini',
                'messages': [
                    {'role': 'system', 'content': '–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏. –û—Ç–≤–µ—á–∞–µ—à—å —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–º JSON.'},
                    {'role': 'user', 'content': prompt}
                ],
                'temperature': 0.3,
                'response_format': {'type': 'json_object'}
            },
            timeout=60
        )
        
        if response.status_code == 200:
            data = response.json()
            content = data['choices'][0]['message']['content']
            result = json.loads(content)
            clusters = result.get('clusters', [])
            print(f'[OPENAI] Created {len(clusters)} clusters via GPT-4o-mini')
            return clusters
        else:
            print(f'[OPENAI] API error {response.status_code}: {response.text}, falling back to TF-IDF')
            return smart_clusterize(phrases, mode)
            
    except Exception as e:
        print(f'[OPENAI] Error: {str(e)}, falling back to TF-IDF')
        return smart_clusterize(phrases, mode)

def smart_clusterize(phrases: List[Dict[str, Any]], mode: str = 'seo') -> List[Dict[str, Any]]:
    '''
    –°—É–ø–µ—Ä-–ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å —Ä–∞–∑–Ω—ã–º–∏ —Ä–µ–∂–∏–º–∞–º–∏:
    - mode='seo': –®–∏—Ä–æ–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã (10-30 —Ñ—Ä–∞–∑) –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    - mode='context': –ë–∞–∑–æ–≤—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã (10-50 —Ñ—Ä–∞–∑) –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π —Ä–µ–∫–ª–∞–º—ã
    '''
    if not phrases:
        return []
    
    competitors = ['–∞–≤–∏—Ç–æ', '—Ü–∏–∞–Ω', '–¥–æ–º–∫–ª–∏–∫', '—è–Ω–¥–µ–∫—Å –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å', '—é–ª–∞', '–∏–∑ —Ä—É–∫ –≤ —Ä—É–∫–∏']
    competitor_phrases = []
    regular_phrases = []
    
    for p in phrases:
        is_competitor = any(comp in p['phrase'].lower() for comp in competitors)
        if is_competitor:
            competitor_phrases.append(p)
        else:
            regular_phrases.append(p)
    
    if mode == 'context':
        similarity_threshold = 0.05
        target_clusters_ratio = 25
    else:
        similarity_threshold = 0.08
        target_clusters_ratio = 15
    
    phrases = regular_phrases
    
    if len(phrases) < 5:
        return [{
            'cluster_name': '–í—Å–µ –∑–∞–ø—Ä–æ—Å—ã',
            'total_count': sum(p['count'] for p in phrases),
            'phrases_count': len(phrases),
            'avg_words': round(sum(len(p['phrase'].split()) for p in phrases) / len(phrases), 1),
            'max_frequency': max(p['count'] for p in phrases),
            'min_frequency': min(p['count'] for p in phrases),
            'intent': detect_intent(phrases[0]['phrase']),
            'phrases': sorted(phrases, key=lambda x: x['count'], reverse=True)
        }]
    
    lemmatized = [lemmatize_phrase(p['phrase']) for p in phrases]
    
    tfidf_vectors = calculate_tfidf(lemmatized)
    
    n = len(phrases)
    similarity_matrix = [[0.0] * n for _ in range(n)]
    
    for i in range(n):
        for j in range(i+1, n):
            sim = cosine_similarity_simple(tfidf_vectors[i], tfidf_vectors[j])
            similarity_matrix[i][j] = sim
            similarity_matrix[j][i] = sim
    
    clusters = [[i] for i in range(n)]
    
    target_clusters = max(3, min(20, n // target_clusters_ratio))
    
    while len(clusters) > target_clusters:
        max_sim = -1
        merge_pair = None
        
        for i in range(len(clusters)):
            for j in range(i+1, len(clusters)):
                avg_sim = 0
                count = 0
                for idx_i in clusters[i]:
                    for idx_j in clusters[j]:
                        avg_sim += similarity_matrix[idx_i][idx_j]
                        count += 1
                
                if count > 0:
                    avg_sim /= count
                    if avg_sim > max_sim:
                        max_sim = avg_sim
                        merge_pair = (i, j)
        
        if max_sim < similarity_threshold or merge_pair is None:
            break
        
        i, j = merge_pair
        clusters[i].extend(clusters[j])
        del clusters[j]
    
    stop_words_for_naming = {
        '–∫—É–ø–∏—Ç—å', '–∑–∞–∫–∞–∑–∞—Ç—å', '—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–Ω–µ–¥–æ—Ä–æ–≥–æ', 
        '–¥–µ—à–µ–≤–æ', '–º–æ—Å–∫–≤–∞', '—Å–ø–±', '—Ä–æ—Å—Å–∏—è', '–¥–æ—Å—Ç–∞–≤–∫–∞'
    }
    
    result = []
    for cluster_indices in clusters:
        cluster_phrases = [phrases[i] for i in cluster_indices]
        sorted_phrases = sorted(cluster_phrases, key=lambda x: x['count'], reverse=True)
        
        lemmas_counter = defaultdict(int)
        for idx in cluster_indices:
            lemma = lemmatized[idx]
            for word in lemma.split():
                if len(word) > 2 and word not in stop_words_for_naming:
                    lemmas_counter[word] += 1
        
        if lemmas_counter:
            sorted_words = sorted(lemmas_counter.items(), key=lambda x: x[1], reverse=True)
            top_words = [w for w, _ in sorted_words[:2]]
            cluster_name = ' '.join(top_words).title()
        else:
            words = sorted_phrases[0]['phrase'].split()
            cluster_name = ' '.join(words[:2]).title()
        
        intents = [detect_intent(p['phrase']) for p in cluster_phrases]
        intent_counts = defaultdict(int)
        for intent in intents:
            intent_counts[intent] += 1
        dominant_intent = max(intent_counts.items(), key=lambda x: x[1])[0]
        
        result.append({
            'cluster_name': cluster_name,
            'total_count': sum(p['count'] for p in cluster_phrases),
            'phrases_count': len(cluster_phrases),
            'avg_words': round(sum(len(p['phrase'].split()) for p in cluster_phrases) / len(cluster_phrases), 1),
            'max_frequency': max(p['count'] for p in cluster_phrases),
            'min_frequency': min(p['count'] for p in cluster_phrases),
            'intent': dominant_intent,
            'phrases': sorted_phrases
        })
    
    result.sort(key=lambda x: x['total_count'], reverse=True)
    
    if competitor_phrases:
        result.append({
            'cluster_name': '–ê–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã –∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã',
            'total_count': sum(p['count'] for p in competitor_phrases),
            'phrases_count': len(competitor_phrases),
            'avg_words': round(sum(len(p['phrase'].split()) for p in competitor_phrases) / len(competitor_phrases), 1),
            'max_frequency': max(p['count'] for p in competitor_phrases),
            'min_frequency': min(p['count'] for p in competitor_phrases),
            'intent': 'general',
            'phrases': sorted(competitor_phrases, key=lambda x: x['count'], reverse=True)
        })
    
    return result

def handler(event: Dict[str, Any], context: Any) -> Dict[str, Any]:
    '''
    Business: –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –Ø–Ω–¥–µ–∫—Å.Wordstat API —Å –°–£–ü–ï–† —É–º–Ω–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–µ–π
    Args: event - dict —Å httpMethod, body (keywords: List[str], regions: List[int])
          context - –æ–±—ä–µ–∫—Ç —Å –∞—Ç—Ä–∏–±—É—Ç–∞–º–∏ request_id, function_name
    Returns: HTTP response —Å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –æ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
    '''
    method: str = event.get('httpMethod', 'GET')
    
    if method == 'OPTIONS':
        return {
            'statusCode': 200,
            'headers': {
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
                'Access-Control-Allow-Headers': 'Content-Type, X-Auth-Token',
                'Access-Control-Max-Age': '86400'
            },
            'body': ''
        }
    
    token = os.environ.get('YANDEX_WORDSTAT_TOKEN')
    if not token:
        return {
            'statusCode': 500,
            'headers': {
                'Content-Type': 'application/json',
                'Access-Control-Allow-Origin': '*'
            },
            'isBase64Encoded': False,
            'body': json.dumps({'error': '–¢–æ–∫–µ–Ω –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω. –î–æ–±–∞–≤—å—Ç–µ YANDEX_WORDSTAT_TOKEN.'})
        }
    
    if method == 'GET':
        try:
            api_url = 'https://api.wordstat.yandex.net/v1/getRegionsTree'
            headers = {
                'Authorization': f'Bearer {token}',
                'Content-Type': 'application/json; charset=utf-8',
                'Accept-Language': 'ru'
            }
            
            response = requests.get(api_url, headers=headers, timeout=30)
            
            if response.status_code != 200:
                return {
                    'statusCode': response.status_code,
                    'headers': {
                        'Content-Type': 'application/json',
                        'Access-Control-Allow-Origin': '*'
                    },
                    'isBase64Encoded': False,
                    'body': json.dumps({'error': f'API error: {response.status_code}'})
                }
            
            data = response.json()
            
            return {
                'statusCode': 200,
                'headers': {
                    'Content-Type': 'application/json',
                    'Access-Control-Allow-Origin': '*'
                },
                'isBase64Encoded': False,
                'body': json.dumps({'success': True, 'regions': data})
            }
        except Exception as e:
            print(f'[REGIONS ERROR] {str(e)}')
            return {
                'statusCode': 500,
                'headers': {
                    'Content-Type': 'application/json',
                    'Access-Control-Allow-Origin': '*'
                },
                'isBase64Encoded': False,
                'body': json.dumps({'error': str(e)})
            }
    
    if method == 'POST':
        body_str = event.get('body', '{}')
        if not body_str or body_str.strip() == '':
            body_str = '{}'
        body_data = json.loads(body_str)
        keywords: List[str] = body_data.get('keywords', [])
        regions: List[int] = body_data.get('regions', [213])
        use_openai: bool = body_data.get('use_openai', True)
        
        if not keywords:
            return {
                'statusCode': 400,
                'headers': {
                    'Content-Type': 'application/json',
                    'Access-Control-Allow-Origin': '*'
                },
                'isBase64Encoded': False,
                'body': json.dumps({'error': '–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞'})
            }
        
        api_url = 'https://api.wordstat.yandex.net/v1/topRequests'
        headers = {
            'Authorization': f'Bearer {token}',
            'Content-Type': 'application/json; charset=utf-8',
            'Accept-Language': 'ru'
        }
        
        try:
            payload = {
                'phrase': keywords[0],
                'regions': regions
            }
            
            print(f'[WORDSTAT] Request payload: phrase={keywords[0]}, regions={regions}')
            
            response = requests.post(api_url, json=payload, headers=headers, timeout=30)
            
            if response.status_code != 200:
                return {
                    'statusCode': response.status_code,
                    'headers': {
                        'Content-Type': 'application/json',
                        'Access-Control-Allow-Origin': '*'
                    },
                    'isBase64Encoded': False,
                    'body': json.dumps({'error': f'API error: {response.status_code}'})
                }
            
            data = response.json()
            
            if 'error' in data:
                return {
                    'statusCode': 400,
                    'headers': {
                        'Content-Type': 'application/json',
                        'Access-Control-Allow-Origin': '*'
                    },
                    'isBase64Encoded': False,
                    'body': json.dumps({'error': data.get('error')})
                }
            
            top_requests = data.get('topRequests', [])
            clustering_mode = body_data.get('mode', 'seo')
            print(f'[WORDSTAT] Got {len(top_requests)} phrases from Yandex API, mode: {clustering_mode}')
            
            if use_openai:
                print('[WORDSTAT] Using OpenAI GPT-4o-mini for clustering')
                clusters = clusterize_with_openai(top_requests, mode=clustering_mode)
            else:
                print('[WORDSTAT] Using TF-IDF for clustering')
                clusters = smart_clusterize(top_requests, mode=clustering_mode)
            
            print(f'[WORDSTAT] Created {len(clusters)} smart clusters ({clustering_mode} mode)')
            
            minus_words = {}
            if clustering_mode == 'context':
                minus_words = detect_minus_words(top_requests)
                print(f'[WORDSTAT] Detected {sum(v["count"] for v in minus_words.values())} minus-words')
            
            search_query = [{
                'Keyword': keywords[0],
                'Shows': top_requests[0]['count'] if top_requests else 0,
                'TopRequests': top_requests,
                'Clusters': clusters,
                'MinusWords': minus_words,
                'Mode': clustering_mode
            }]
        
        except requests.exceptions.Timeout:
            return {
                'statusCode': 504,
                'headers': {
                    'Content-Type': 'application/json',
                    'Access-Control-Allow-Origin': '*'
                },
                'isBase64Encoded': False,
                'body': json.dumps({'error': '–ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç API'})
            }
        except Exception as e:
            print(f'[WORDSTAT ERROR] {str(e)}')
            import traceback
            traceback.print_exc()
            return {
                'statusCode': 500,
                'headers': {
                    'Content-Type': 'application/json',
                    'Access-Control-Allow-Origin': '*'
                },
                'isBase64Encoded': False,
                'body': json.dumps({'error': f'–û—à–∏–±–∫–∞: {str(e)}'})
            }
        
        return {
            'statusCode': 200,
            'headers': {
                'Content-Type': 'application/json',
                'Access-Control-Allow-Origin': '*'
            },
            'isBase64Encoded': False,
            'body': json.dumps({
                'success': True,
                'data': {
                    'SearchQuery': search_query
                }
            }, ensure_ascii=False)
        }
    
    return {
        'statusCode': 405,
        'headers': {
            'Content-Type': 'application/json',
            'Access-Control-Allow-Origin': '*'
        },
        'isBase64Encoded': False,
        'body': json.dumps({'error': 'Method not allowed'})
    }