import json
import os
from typing import Dict, Any, List
import requests
from collections import defaultdict
import math
import pymorphy3

morph = pymorphy3.MorphAnalyzer()

def lemmatize_phrase(phrase: str) -> str:
    '''–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ñ—Ä–∞–∑—ã ‚Äî –ø—Ä–∏–≤–æ–¥–∏—Ç —Å–ª–æ–≤–∞ –∫ –Ω–∞—á–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ'''
    words = phrase.lower().split()
    lemmas = []
    for word in words:
        parsed = morph.parse(word)[0]
        lemmas.append(parsed.normal_form)
    return ' '.join(lemmas)

def detect_intent(phrase: str) -> str:
    '''–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π –∏–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π intent'''
    commercial_markers = [
        '–∫—É–ø–∏—Ç—å', '—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–∑–∞–∫–∞–∑–∞—Ç—å', '–¥–æ—Å—Ç–∞–≤–∫–∞', 
        '–Ω–µ–¥–æ—Ä–æ–≥–æ', '–¥–µ—à–µ–≤–æ', '–º–∞–≥–∞–∑–∏–Ω', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç', '—Å–∞–π—Ç',
        '–ø—Ä–æ–¥–∞–∂–∞', '—Å–∫–∏–¥–∫–∞', '–∞–∫—Ü–∏—è', '–æ—Ñ–∏—Å', '—Ç–µ–ª–µ—Ñ–æ–Ω'
    ]
    
    info_markers = [
        '–∫–∞–∫', '—á—Ç–æ', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∫–∞–∫–æ–π',
        '–æ—Ç–∑—ã–≤—ã', '—Ä–µ–π—Ç–∏–Ω–≥', '–ª—É—á—à–∏–π', '—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ', '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è',
        '—Ñ–æ—Ç–æ', '–≤–∏–¥–µ–æ', '—Å—Ç–∞—Ç—å—è', '—Ñ–æ—Ä—É–º'
    ]
    
    phrase_lower = phrase.lower()
    
    commercial_score = sum(1 for marker in commercial_markers if marker in phrase_lower)
    info_score = sum(1 for marker in info_markers if marker in phrase_lower)
    
    if commercial_score > info_score:
        return 'commercial'
    elif info_score > commercial_score:
        return 'informational'
    else:
        return 'general'

def detect_minus_words(phrases: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
    '''
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–∏–Ω—É—Å-—Å–ª–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π —Ä–µ–∫–ª–∞–º—ã
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –Ω–µ—Ü–µ–ª–µ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    '''
    minus_categories = {
        'free': {
            'name': 'üÜì –ë–µ—Å–ø–ª–∞—Ç–Ω–æ / –•–∞–ª—è–≤–∞',
            'keywords': ['–±–µ—Å–ø–ª–∞—Ç–Ω–æ', '–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π', '–¥–∞—Ä–æ–º', '–±–µ–∑–≤–æ–∑–º–µ–∑–¥–Ω–æ', '–∑–∞–¥–∞—Ä–º–∞', 'free'],
            'phrases': []
        },
        'diy': {
            'name': 'üîß –°–≤–æ–∏–º–∏ —Ä—É–∫–∞–º–∏ / DIY',
            'keywords': ['—Å–≤–æ–∏–º–∏ —Ä—É–∫–∞–º–∏', '—Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ', '—Å–∞–º', '—Å–∞–º–æ–º—É', 'diy', '–∫–∞–∫ —Å–¥–µ–ª–∞—Ç—å', '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è'],
            'phrases': []
        },
        'competitors': {
            'name': 'üè¢ –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã / –ü–ª–æ—â–∞–¥–∫–∏',
            'keywords': ['–∞–≤–∏—Ç–æ', '—Ü–∏–∞–Ω', '–¥–æ–º–∫–ª–∏–∫', '—è–Ω–¥–µ–∫—Å –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å', '—é–ª–∞', '–∏–∑ —Ä—É–∫ –≤ —Ä—É–∫–∏'],
            'phrases': []
        },
        'info': {
            'name': '‚ÑπÔ∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã',
            'keywords': ['—á—Ç–æ —Ç–∞–∫–æ–µ', '–∫–∞–∫ –≤—ã–±—Ä–∞—Ç—å', '–∫–∞–∫–æ–π –ª—É—á—à–µ', '–æ—Ç–ª–∏—á–∏—è', '—Ä–∞–∑–Ω–∏—Ü–∞', '–ø–ª—é—Å—ã –º–∏–Ω—É—Å—ã', '—Å–æ–≤–µ—Ç—ã'],
            'phrases': []
        },
        'job': {
            'name': 'üíº –†–∞–±–æ—Ç–∞ / –í–∞–∫–∞–Ω—Å–∏–∏',
            'keywords': ['–≤–∞–∫–∞–Ω—Å–∏–∏', '—Ä–∞–±–æ—Ç–∞', '—Ä–µ–∑—é–º–µ', '–∑–∞—Ä–ø–ª–∞—Ç–∞', '—Ç—Ä–µ–±—É—é—Ç—Å—è', '–∏—â—É —Ä–∞–±–æ—Ç—É', '–∫–∞—Ä—å–µ—Ä–∞'],
            'phrases': []
        },
        'education': {
            'name': 'üéì –û–±—É—á–µ–Ω–∏–µ / –ö—É—Ä—Å—ã',
            'keywords': ['–∫—É—Ä—Å—ã', '–æ–±—É—á–µ–Ω–∏–µ', '—Å–µ–º–∏–Ω–∞—Ä', '—Ç—Ä–µ–Ω–∏–Ω–≥', '–≤–µ–±–∏–Ω–∞—Ä', '–º–∞—Å—Ç–µ—Ä –∫–ª–∞—Å—Å', '—É—Ä–æ–∫–∏'],
            'phrases': []
        },
        'download': {
            'name': 'üì• –°–∫–∞—á–∞—Ç—å / –ó–∞–≥—Ä—É–∑–∏—Ç—å',
            'keywords': ['—Å–∫–∞—á–∞—Ç—å', '–∑–∞–≥—Ä—É–∑–∏—Ç—å', 'download', '—Ç–æ—Ä—Ä–µ–Ω—Ç', '–æ–Ω–ª–∞–π–Ω', '—Å–º–æ—Ç—Ä–µ—Ç—å'],
            'phrases': []
        },
        'porn': {
            'name': 'üîû –í–∑—Ä–æ—Å–ª—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç',
            'keywords': ['–ø–æ—Ä–Ω–æ', '—Å–µ–∫—Å', 'xxx', '—ç—Ä–æ—Ç–∏–∫–∞', '–∏–Ω—Ç–∏–º'],
            'phrases': []
        },
        'other': {
            'name': '‚ùì –ü—Ä–æ—á–∏–µ –Ω–µ—Ü–µ–ª–µ–≤—ã–µ',
            'keywords': ['–∏–≥—Ä–∞', '–∏–≥—Ä—ã', '–º—É–ª—å—Ç—Ñ–∏–ª—å–º', '–∫–∞—Ä—Ç–∏–Ω–∫–∏', '—Ä–∏—Å—É–Ω–æ–∫', '—Ä–∞—Å–∫—Ä–∞—Å–∫–∞', '—à—É—Ç–∫–∏', '–∞–Ω–µ–∫–¥–æ—Ç—ã'],
            'phrases': []
        }
    }
    
    for phrase_data in phrases:
        phrase_lower = phrase_data['phrase'].lower()
        
        matched = False
        for category_key, category_data in minus_categories.items():
            for keyword in category_data['keywords']:
                if keyword in phrase_lower:
                    category_data['phrases'].append(phrase_data)
                    matched = True
                    break
            if matched:
                break
    
    result = {}
    for key, data in minus_categories.items():
        if len(data['phrases']) > 0:
            result[key] = {
                'name': data['name'],
                'count': len(data['phrases']),
                'total_volume': sum(p['count'] for p in data['phrases']),
                'phrases': sorted(data['phrases'], key=lambda x: x['count'], reverse=True)
            }
    
    return result

def calculate_tfidf(phrases: List[str]) -> List[Dict[str, float]]:
    '''–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è TF-IDF –±–µ–∑ scikit-learn'''
    stop_words = {
        '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–∏–∑', '–∏', '–∏–ª–∏', '–∫–∞–∫', '—á—Ç–æ', '–∑–∞',
        '—ç—Ç–æ', '—Ç–æ', '—Ç–∞–∫', '–Ω–æ', '–∞', '–æ', '—É', '–æ—Ç', '–∫', '–¥–æ', '–ø—Ä–∏',
        '–±–µ–∑', '–ø–æ–¥', '–Ω–∞–¥', '–º–µ–∂–¥—É', '–ø–µ—Ä–µ–¥', '—á–µ—Ä–µ–∑', '–ø–æ—Å–ª–µ'
    }
    
    doc_words = []
    for phrase in phrases:
        words = [w for w in phrase.lower().split() if w not in stop_words and len(w) > 2]
        doc_words.append(words)
    
    word_doc_count = defaultdict(int)
    for words in doc_words:
        unique_words = set(words)
        for word in unique_words:
            word_doc_count[word] += 1
    
    n_docs = len(phrases)
    idf = {}
    for word, count in word_doc_count.items():
        idf[word] = math.log(n_docs / count)
    
    tfidf_vectors = []
    for words in doc_words:
        word_count = defaultdict(int)
        for word in words:
            word_count[word] += 1
        
        tf = {w: count / len(words) if len(words) > 0 else 0 for w, count in word_count.items()}
        
        tfidf = {w: tf[w] * idf.get(w, 0) for w in tf}
        tfidf_vectors.append(tfidf)
    
    return tfidf_vectors

def cosine_similarity_simple(vec1: Dict[str, float], vec2: Dict[str, float]) -> float:
    '''–ö–æ—Å–∏–Ω—É—Å–Ω–∞—è –±–ª–∏–∑–æ—Å—Ç—å –º–µ–∂–¥—É –¥–≤—É–º—è –≤–µ–∫—Ç–æ—Ä–∞–º–∏'''
    all_words = set(vec1.keys()) | set(vec2.keys())
    
    dot_product = sum(vec1.get(w, 0) * vec2.get(w, 0) for w in all_words)
    
    norm1 = math.sqrt(sum(v**2 for v in vec1.values()))
    norm2 = math.sqrt(sum(v**2 for v in vec2.values()))
    
    if norm1 == 0 or norm2 == 0:
        return 0.0
    
    return dot_product / (norm1 * norm2)

def clusterize_with_openai(phrases: List[Dict[str, Any]], mode: str = 'context') -> tuple:
    '''
    –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ OpenAI GPT-4o-mini —Å –ø—Ä–æ–∫—Å–∏
    Returns: (clusters, minus_words)
    '''
    openai_key = os.environ.get('OPENAI_API_KEY')
    if not openai_key:
        print('[OPENAI] API key not found - using TF-IDF clustering')
        clusters = smart_clusterize(phrases, mode)
        minus_words = detect_minus_words(phrases) if mode == 'context' else {}
        return clusters, minus_words
    
    phrases_text = '\n'.join([f"{p['phrase']} ({p['count']} –ø–æ–∫–∞–∑–æ–≤)" for p in phrases[:100]])
    
    if mode == 'context':
        prompt = f"""–ö–ª–∞—Å—Ç–µ—Ä–∏–∑—É–π –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –Ø–Ω–¥–µ–∫—Å.–î–∏—Ä–µ–∫—Ç.

–ü–†–ê–í–ò–õ–ê:
1. –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û —Ñ—Ä–∞–∑—ã –∏–∑ —Å–ø–∏—Å–∫–∞ (–Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–æ–≤—ã–µ)
2. –£–∑–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã 3-10 —Ñ—Ä–∞–∑
3. –†–∞–∑–¥–µ–ª—è–π –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º (—Ç–∏–ø—ã, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏, —Ä–∞–∑–º–µ—Ä—ã)
4. –ù–∞–∑–≤–∞–Ω–∏–µ = 2-3 —Å–ª–æ–≤–∞ –∏–∑ —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö —Ñ—Ä–∞–∑ –∫–ª–∞—Å—Ç–µ—Ä–∞

–ú–ò–ù–£–°-–°–õ–û–í–ê (–Ω–µ—Ü–µ–ª–µ–≤—ã–µ):
- free: –±–µ—Å–ø–ª–∞—Ç–Ω–æ, –¥–∞—Ä–æ–º
- info: –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è, —Å–≤–æ–∏–º–∏ —Ä—É–∫–∞–º–∏, –æ—Ç–∑—ã–≤—ã
- competitors: –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã, –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å—ã
- irrelevant: –≤–∞–∫–∞–Ω—Å–∏–∏, –∫—É—Ä—Å—ã, –Ω–µ—Å–≤—è–∑–∞–Ω–Ω–æ–µ

–§—Ä–∞–∑—ã:
{phrases_text}

JSON:
{{
  "clusters": [{{"cluster_name": "...", "intent": "commercial/informational", "phrases": [{{"phrase": "...", "count": N}}]}}],
  "minus_words": {{"free": [], "info": [], "competitors": [], "irrelevant": []}}
}}"""
    else:
        prompt = f"""–ö–ª–∞—Å—Ç–µ—Ä–∏–∑—É–π –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è SEO –ø–æ—Å–∞–¥–æ—á–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü.

–ü–†–ê–í–ò–õ–ê:
1. –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û —Ñ—Ä–∞–∑—ã –∏–∑ —Å–ø–∏—Å–∫–∞
2. –°—Ä–µ–¥–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã 10-30 —Ñ—Ä–∞–∑
3. –ì—Ä—É–ø–ø–∏—Ä—É–π –ø–æ –∏–Ω—Ç–µ–Ω—Ç—É –∏ —Å–º—ã—Å–ª—É
4. –ù–∞–∑–≤–∞–Ω–∏–µ = 2-4 —Å–ª–æ–≤–∞ –∏–∑ —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö —Ñ—Ä–∞–∑

–§—Ä–∞–∑—ã:
{phrases_text}

JSON:
{{
  "clusters": [{{"cluster_name": "...", "intent": "commercial/informational", "phrases": [{{"phrase": "...", "count": N}}]}}]
}}"""
    
    proxy_url = os.environ.get('OPENAI_PROXY_URL')
    proxies = {}
    if proxy_url:
        proxies = {
            'http': proxy_url,
            'https': proxy_url
        }
        print(f'[OPENAI] Using proxy: {proxy_url[:20]}...')
    
    try:
        response = requests.post(
            'https://api.openai.com/v1/chat/completions',
            headers={
                'Authorization': f'Bearer {openai_key}',
                'Content-Type': 'application/json'
            },
            json={
                'model': 'gpt-4o',
                'messages': [
                    {'role': 'system', 'content': '–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—à—å —Å–º—ã—Å–ª –∏ –Ω–∞–º–µ—Ä–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –û—Ç–≤–µ—á–∞–µ—à—å —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–º JSON.'},
                    {'role': 'user', 'content': prompt}
                ],
                'temperature': 0.3,
                'response_format': {'type': 'json_object'}
            },
            proxies=proxies if proxies else None,
            timeout=90
        )
        
        if response.status_code == 200:
            data = response.json()
            content = data['choices'][0]['message']['content']
            result = json.loads(content)
            clusters = result.get('clusters', [])
            minus_words = result.get('minus_words', {})
            print(f'[OPENAI] Created {len(clusters)} clusters via GPT-4o-mini')
            if minus_words:
                total_minus = sum(len(v) for v in minus_words.values() if isinstance(v, list))
                print(f'[OPENAI] Detected {total_minus} minus-words')
            return clusters, minus_words
        else:
            print(f'[OPENAI] API error {response.status_code}: {response.text}, falling back to TF-IDF')
            clusters = smart_clusterize(phrases, mode)
            minus_words = detect_minus_words(phrases) if mode == 'context' else {}
            return clusters, minus_words
            
    except Exception as e:
        print(f'[OPENAI] Error: {str(e)}, falling back to TF-IDF')
        clusters = smart_clusterize(phrases, mode)
        minus_words = detect_minus_words(phrases) if mode == 'context' else {}
        return clusters, minus_words

def smart_clusterize(phrases: List[Dict[str, Any]], mode: str = 'seo') -> List[Dict[str, Any]]:
    '''
    –°—É–ø–µ—Ä-–ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å —Ä–∞–∑–Ω—ã–º–∏ —Ä–µ–∂–∏–º–∞–º–∏:
    - mode='seo': –®–∏—Ä–æ–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã (10-30 —Ñ—Ä–∞–∑) –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    - mode='context': –ë–∞–∑–æ–≤—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã (10-50 —Ñ—Ä–∞–∑) –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π —Ä–µ–∫–ª–∞–º—ã
    '''
    if not phrases:
        return []
    
    competitors = ['–∞–≤–∏—Ç–æ', '—Ü–∏–∞–Ω', '–¥–æ–º–∫–ª–∏–∫', '—è–Ω–¥–µ–∫—Å –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å', '—é–ª–∞', '–∏–∑ —Ä—É–∫ –≤ —Ä—É–∫–∏']
    competitor_phrases = []
    regular_phrases = []
    
    for p in phrases:
        is_competitor = any(comp in p['phrase'].lower() for comp in competitors)
        if is_competitor:
            competitor_phrases.append(p)
        else:
            regular_phrases.append(p)
    
    if mode == 'context':
        similarity_threshold = 0.05
        target_clusters_ratio = 25
    else:
        similarity_threshold = 0.08
        target_clusters_ratio = 15
    
    phrases = regular_phrases
    
    if len(phrases) < 5:
        return [{
            'cluster_name': '–í—Å–µ –∑–∞–ø—Ä–æ—Å—ã',
            'total_count': sum(p['count'] for p in phrases),
            'phrases_count': len(phrases),
            'avg_words': round(sum(len(p['phrase'].split()) for p in phrases) / len(phrases), 1),
            'max_frequency': max(p['count'] for p in phrases),
            'min_frequency': min(p['count'] for p in phrases),
            'intent': detect_intent(phrases[0]['phrase']),
            'phrases': sorted(phrases, key=lambda x: x['count'], reverse=True)
        }]
    
    lemmatized = [lemmatize_phrase(p['phrase']) for p in phrases]
    
    tfidf_vectors = calculate_tfidf(lemmatized)
    
    n = len(phrases)
    similarity_matrix = [[0.0] * n for _ in range(n)]
    
    for i in range(n):
        for j in range(i+1, n):
            sim = cosine_similarity_simple(tfidf_vectors[i], tfidf_vectors[j])
            similarity_matrix[i][j] = sim
            similarity_matrix[j][i] = sim
    
    clusters = [[i] for i in range(n)]
    
    target_clusters = max(3, min(20, n // target_clusters_ratio))
    
    while len(clusters) > target_clusters:
        max_sim = -1
        merge_pair = None
        
        for i in range(len(clusters)):
            for j in range(i+1, len(clusters)):
                avg_sim = 0
                count = 0
                for idx_i in clusters[i]:
                    for idx_j in clusters[j]:
                        avg_sim += similarity_matrix[idx_i][idx_j]
                        count += 1
                
                if count > 0:
                    avg_sim /= count
                    if avg_sim > max_sim:
                        max_sim = avg_sim
                        merge_pair = (i, j)
        
        if max_sim < similarity_threshold or merge_pair is None:
            break
        
        i, j = merge_pair
        clusters[i].extend(clusters[j])
        del clusters[j]
    
    stop_words_for_naming = {
        '–∫—É–ø–∏—Ç—å', '–∑–∞–∫–∞–∑–∞—Ç—å', '—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–Ω–µ–¥–æ—Ä–æ–≥–æ', 
        '–¥–µ—à–µ–≤–æ', '–º–æ—Å–∫–≤–∞', '—Å–ø–±', '—Ä–æ—Å—Å–∏—è', '–¥–æ—Å—Ç–∞–≤–∫–∞'
    }
    
    result = []
    for cluster_indices in clusters:
        cluster_phrases = [phrases[i] for i in cluster_indices]
        sorted_phrases = sorted(cluster_phrases, key=lambda x: x['count'], reverse=True)
        
        lemmas_counter = defaultdict(int)
        for idx in cluster_indices:
            lemma = lemmatized[idx]
            for word in lemma.split():
                if len(word) > 2 and word not in stop_words_for_naming:
                    lemmas_counter[word] += 1
        
        if lemmas_counter:
            sorted_words = sorted(lemmas_counter.items(), key=lambda x: x[1], reverse=True)
            top_words = [w for w, _ in sorted_words[:2]]
            cluster_name = ' '.join(top_words).title()
        else:
            words = sorted_phrases[0]['phrase'].split()
            cluster_name = ' '.join(words[:2]).title()
        
        intents = [detect_intent(p['phrase']) for p in cluster_phrases]
        intent_counts = defaultdict(int)
        for intent in intents:
            intent_counts[intent] += 1
        dominant_intent = max(intent_counts.items(), key=lambda x: x[1])[0]
        
        result.append({
            'cluster_name': cluster_name,
            'total_count': sum(p['count'] for p in cluster_phrases),
            'phrases_count': len(cluster_phrases),
            'avg_words': round(sum(len(p['phrase'].split()) for p in cluster_phrases) / len(cluster_phrases), 1),
            'max_frequency': max(p['count'] for p in cluster_phrases),
            'min_frequency': min(p['count'] for p in cluster_phrases),
            'intent': dominant_intent,
            'phrases': sorted_phrases
        })
    
    result.sort(key=lambda x: x['total_count'], reverse=True)
    
    if competitor_phrases:
        result.append({
            'cluster_name': '–ê–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã –∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã',
            'total_count': sum(p['count'] for p in competitor_phrases),
            'phrases_count': len(competitor_phrases),
            'avg_words': round(sum(len(p['phrase'].split()) for p in competitor_phrases) / len(competitor_phrases), 1),
            'max_frequency': max(p['count'] for p in competitor_phrases),
            'min_frequency': min(p['count'] for p in competitor_phrases),
            'intent': 'general',
            'phrases': sorted(competitor_phrases, key=lambda x: x['count'], reverse=True)
        })
    
    return result

def generate_geo_keywords(address: str, base_query: str) -> List[str]:
    '''
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥–µ–æ–∑–∞–≤–∏—Å–∏–º—ã—Ö –≤–∞—Ä–∏–∞—Ü–∏–π –∞–¥—Ä–µ—Å–∞ —á–µ—Ä–µ–∑ OpenAI
    address: "–°—Ç–∞–≤—Ä–æ–ø–æ–ª—å, –ö—É–ª–∞–∫–æ–≤–∞ 1"
    base_query: "–∫—É–ø–∏—Ç—å –∫–≤–∞—Ä—Ç–∏—Ä—É"
    Returns: ["–∫—É–ø–∏—Ç—å –∫–≤–∞—Ä—Ç–∏—Ä—É –ö—É–ª–∞–∫–æ–≤–∞", "–∫—É–ø–∏—Ç—å –∫–≤–∞—Ä—Ç–∏—Ä—É –ö—É–ª–∞–∫–æ–≤–∞ 1", ...]
    '''
    openai_key = os.environ.get('OPENAI_API_KEY')
    proxy_url = os.environ.get('OPENAI_PROXY_URL')
    
    if not openai_key:
        print('[GEO] OpenAI key not found')
        return []
    
    prompt = f"""–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ –∏ –≥–µ–æ–ª–æ–∫–∞—Ü–∏–∏. –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –í–°–ï –≤–æ–∑–º–æ–∂–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Ñ—Ä–∞–∑ –¥–ª—è —ç—Ç–æ–≥–æ –∞–¥—Ä–µ—Å–∞:

–ê–¥—Ä–µ—Å: {address}
–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å: {base_query}

–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π 15-25 –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ª—é–¥–∏ –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–∏ –ø–æ–∏—Å–∫–µ:
1. –ü–æ–ª–Ω—ã–π –∞–¥—Ä–µ—Å: "–≥–æ—Ä–æ–¥ —É–ª–∏—Ü–∞ –¥–æ–º"
2. –£–ª–∏—Ü–∞ —Å –¥–æ–º–æ–º: "—É–ª–∏—Ü–∞ –¥–æ–º"  
3. –¢–æ–ª—å–∫–æ —É–ª–∏—Ü–∞: "—É–ª–∏—Ü–∞"
4. –†–∞–π–æ–Ω –≥–æ—Ä–æ–¥–∞
5. –û—Ä–∏–µ–Ω—Ç–∏—Ä—ã —Ä—è–¥–æ–º: "—Ä—è–¥–æ–º —Å [–º–µ—Å—Ç–æ]"
6. –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—ã–µ —É–∑–ª—ã: "—É –º–µ—Ç—Ä–æ [–Ω–∞–∑–≤–∞–Ω–∏–µ]"  
7. –ú–∏–∫—Ä–æ—Ä–∞–π–æ–Ω—ã
8. –†–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: –¢–û–õ–¨–ö–û —Å–ø–∏—Å–æ–∫ —Ñ—Ä–∞–∑ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é, –∫–∞–∂–¥–∞—è —Ñ—Ä–∞–∑–∞ –¥–æ–ª–∂–Ω–∞ –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å –±–∞–∑–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.
–ü—Ä–∏–º–µ—Ä: –∫—É–ø–∏—Ç—å –∫–≤–∞—Ä—Ç–∏—Ä—É –ö—É–ª–∞–∫–æ–≤–∞, –∫—É–ø–∏—Ç—å –∫–≤–∞—Ä—Ç–∏—Ä—É –ö—É–ª–∞–∫–æ–≤–∞ 1, –∫—É–ø–∏—Ç—å –∫–≤–∞—Ä—Ç–∏—Ä—É –°–µ–≤–µ—Ä–æ-–ó–∞–ø–∞–¥–Ω—ã–π —Ä–∞–π–æ–Ω, –∫—É–ø–∏—Ç—å –∫–≤–∞—Ä—Ç–∏—Ä—É —Ä—è–¥–æ–º —Å –¢—É—Ö–∞—á–µ–≤—Å–∫–∏–º —Ä—ã–Ω–∫–æ–º

–û—Ç–≤–µ—Ç:"""

    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {openai_key}'
    }
    
    payload = {
        'model': 'gpt-4o-mini',
        'messages': [{'role': 'user', 'content': prompt}],
        'temperature': 0.7,
        'max_tokens': 400
    }
    
    proxies = None
    if proxy_url:
        proxies = {'http': proxy_url, 'https': proxy_url}
    
    try:
        response = requests.post(
            'https://api.openai.com/v1/chat/completions',
            headers=headers,
            json=payload,
            proxies=proxies,
            timeout=30
        )
        
        if response.status_code != 200:
            print(f'[GEO] OpenAI error: {response.status_code}')
            return []
        
        result = response.json()
        content = result['choices'][0]['message']['content'].strip()
        
        # Parse comma-separated list
        variations = [v.strip() for v in content.split(',') if v.strip()]
        
        print(f'[GEO] Generated {len(variations)} geo variations')
        return variations[:25]  # Limit to 25
        
    except Exception as e:
        print(f'[GEO] Error: {e}')
        return []

def handler(event: Dict[str, Any], context: Any) -> Dict[str, Any]:
    '''
    Business: –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –Ø–Ω–¥–µ–∫—Å.Wordstat API —Å –°–£–ü–ï–† —É–º–Ω–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–µ–π
    Args: event - dict —Å httpMethod, body (keywords: List[str], regions: List[int])
          context - –æ–±—ä–µ–∫—Ç —Å –∞—Ç—Ä–∏–±—É—Ç–∞–º–∏ request_id, function_name
    Returns: HTTP response —Å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –æ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
    '''
    method: str = event.get('httpMethod', 'GET')
    
    if method == 'OPTIONS':
        return {
            'statusCode': 200,
            'headers': {
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
                'Access-Control-Allow-Headers': 'Content-Type, X-Auth-Token',
                'Access-Control-Max-Age': '86400'
            },
            'body': ''
        }
    
    token = os.environ.get('YANDEX_WORDSTAT_TOKEN')
    if not token:
        return {
            'statusCode': 500,
            'headers': {
                'Content-Type': 'application/json',
                'Access-Control-Allow-Origin': '*'
            },
            'isBase64Encoded': False,
            'body': json.dumps({'error': '–¢–æ–∫–µ–Ω –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω. –î–æ–±–∞–≤—å—Ç–µ YANDEX_WORDSTAT_TOKEN.'})
        }
    
    if method == 'GET':
        try:
            api_url = 'https://api.wordstat.yandex.net/v1/getRegionsTree'
            headers = {
                'Authorization': f'Bearer {token}',
                'Content-Type': 'application/json; charset=utf-8',
                'Accept-Language': 'ru'
            }
            
            response = requests.get(api_url, headers=headers, timeout=30)
            
            if response.status_code != 200:
                return {
                    'statusCode': response.status_code,
                    'headers': {
                        'Content-Type': 'application/json',
                        'Access-Control-Allow-Origin': '*'
                    },
                    'isBase64Encoded': False,
                    'body': json.dumps({'error': f'API error: {response.status_code}'})
                }
            
            data = response.json()
            
            return {
                'statusCode': 200,
                'headers': {
                    'Content-Type': 'application/json',
                    'Access-Control-Allow-Origin': '*'
                },
                'isBase64Encoded': False,
                'body': json.dumps({'success': True, 'regions': data})
            }
        except Exception as e:
            print(f'[REGIONS ERROR] {str(e)}')
            return {
                'statusCode': 500,
                'headers': {
                    'Content-Type': 'application/json',
                    'Access-Control-Allow-Origin': '*'
                },
                'isBase64Encoded': False,
                'body': json.dumps({'error': str(e)})
            }
    
    if method == 'POST':
        body_str = event.get('body', '{}')
        if not body_str or body_str.strip() == '':
            body_str = '{}'
        body_data = json.loads(body_str)
        keywords: List[str] = body_data.get('keywords', [])
        regions: List[int] = body_data.get('regions', [213])
        use_openai: bool = body_data.get('use_openai', True)
        object_address: str = body_data.get('objectAddress', '')
        
        print(f'[WORDSTAT] Request params: keywords={keywords}, regions={regions}, use_openai={use_openai}')
        
        if not keywords:
            return {
                'statusCode': 400,
                'headers': {
                    'Content-Type': 'application/json',
                    'Access-Control-Allow-Origin': '*'
                },
                'isBase64Encoded': False,
                'body': json.dumps({'error': '–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞'})
            }
        
        api_url = 'https://api.wordstat.yandex.net/v1/topRequests'
        headers = {
            'Authorization': f'Bearer {token}',
            'Content-Type': 'application/json; charset=utf-8',
            'Accept-Language': 'ru'
        }
        
        try:
            payload = {
                'phrase': keywords[0],
                'regions': regions
            }
            
            print(f'[WORDSTAT] Request payload: phrase={keywords[0]}, regions={regions}')
            
            response = requests.post(api_url, json=payload, headers=headers, timeout=30)
            
            if response.status_code != 200:
                return {
                    'statusCode': response.status_code,
                    'headers': {
                        'Content-Type': 'application/json',
                        'Access-Control-Allow-Origin': '*'
                    },
                    'isBase64Encoded': False,
                    'body': json.dumps({'error': f'API error: {response.status_code}'})
                }
            
            data = response.json()
            
            if 'error' in data:
                return {
                    'statusCode': 400,
                    'headers': {
                        'Content-Type': 'application/json',
                        'Access-Control-Allow-Origin': '*'
                    },
                    'isBase64Encoded': False,
                    'body': json.dumps({'error': data.get('error')})
                }
            
            top_requests = data.get('topRequests', [])
            clustering_mode = body_data.get('mode', 'seo')
            print(f'[WORDSTAT] Got {len(top_requests)} phrases from Yandex API, mode: {clustering_mode}')
            
            if use_openai:
                print('[WORDSTAT] Using OpenAI GPT-4o-mini for clustering')
                clusters, minus_words = clusterize_with_openai(top_requests, mode=clustering_mode)
            else:
                print('[WORDSTAT] Using TF-IDF for clustering')
                clusters = smart_clusterize(top_requests, mode=clustering_mode)
                minus_words = {}
                if clustering_mode == 'context':
                    minus_words = detect_minus_words(top_requests)
            
            print(f'[WORDSTAT] Created {len(clusters)} smart clusters ({clustering_mode} mode)')
            if minus_words:
                total_minus = sum(v.get('count', 0) if isinstance(v, dict) else len(v) if isinstance(v, list) else 0 for v in minus_words.values())
                print(f'[WORDSTAT] Detected {total_minus} minus-words')
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≥–µ–æ–∫–ª—é—á–∏, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –∞–¥—Ä–µ—Å –æ–±—ä–µ–∫—Ç–∞
            geo_cluster = None
            if object_address and object_address.strip():
                print(f'[GEO] Generating geo keywords for: {object_address}')
                geo_keywords = generate_geo_keywords(object_address, keywords[0])
                
                if geo_keywords:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å –≤ Wordstat
                    geo_phrases = []
                    for geo_kw in geo_keywords[:15]:  # Limit to 15 requests
                        try:
                            payload_geo = {'phrase': geo_kw, 'regions': regions}
                            resp_geo = requests.post(api_url, json=payload_geo, headers=headers, timeout=10)
                            if resp_geo.status_code == 200:
                                data_geo = resp_geo.json()
                                top_req_geo = data_geo.get('topRequests', [])
                                if top_req_geo and top_req_geo[0]['count'] > 10:  # Min frequency 10
                                    geo_phrases.append(top_req_geo[0])
                        except:
                            pass
                    
                    if geo_phrases:
                        geo_cluster = {
                            'cluster_name': 'üìç –ì–µ–æ–ª–æ–∫–∞—Ü–∏—è',
                            'total_count': sum(p['count'] for p in geo_phrases),
                            'phrases_count': len(geo_phrases),
                            'avg_words': round(sum(len(p['phrase'].split()) for p in geo_phrases) / len(geo_phrases), 1),
                            'max_frequency': max(p['count'] for p in geo_phrases),
                            'min_frequency': min(p['count'] for p in geo_phrases),
                            'intent': 'commercial',
                            'phrases': sorted(geo_phrases, key=lambda x: x['count'], reverse=True)
                        }
                        clusters.insert(0, geo_cluster)  # Add as first cluster
                        print(f'[GEO] Added geo cluster with {len(geo_phrases)} phrases')
            
            search_query = [{
                'Keyword': keywords[0],
                'Shows': top_requests[0]['count'] if top_requests else 0,
                'TopRequests': top_requests,
                'Clusters': clusters,
                'MinusWords': minus_words,
                'Mode': clustering_mode,
                'GeoCluster': geo_cluster
            }]
        
        except requests.exceptions.Timeout:
            return {
                'statusCode': 504,
                'headers': {
                    'Content-Type': 'application/json',
                    'Access-Control-Allow-Origin': '*'
                },
                'isBase64Encoded': False,
                'body': json.dumps({'error': '–ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç API'})
            }
        except Exception as e:
            print(f'[WORDSTAT ERROR] {str(e)}')
            import traceback
            traceback.print_exc()
            return {
                'statusCode': 500,
                'headers': {
                    'Content-Type': 'application/json',
                    'Access-Control-Allow-Origin': '*'
                },
                'isBase64Encoded': False,
                'body': json.dumps({'error': f'–û—à–∏–±–∫–∞: {str(e)}'})
            }
        
        return {
            'statusCode': 200,
            'headers': {
                'Content-Type': 'application/json',
                'Access-Control-Allow-Origin': '*'
            },
            'isBase64Encoded': False,
            'body': json.dumps({
                'success': True,
                'data': {
                    'SearchQuery': search_query
                }
            }, ensure_ascii=False)
        }
    
    return {
        'statusCode': 405,
        'headers': {
            'Content-Type': 'application/json',
            'Access-Control-Allow-Origin': '*'
        },
        'isBase64Encoded': False,
        'body': json.dumps({'error': 'Method not allowed'})
    }