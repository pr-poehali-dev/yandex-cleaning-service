import json
import os
from typing import Dict, Any, List
import requests
from collections import defaultdict
import math
import pymorphy3
import psycopg2
from psycopg2.extras import RealDictCursor
from datetime import datetime

morph = pymorphy3.MorphAnalyzer()

def check_subscription(user_id: str) -> bool:
    try:
        dsn = os.environ.get('DATABASE_URL')
        conn = psycopg2.connect(dsn, cursor_factory=RealDictCursor)
        cur = conn.cursor()
        
        cur.execute(
            "SELECT * FROM subscriptions WHERE user_id = %s",
            (user_id,)
        )
        subscription = cur.fetchone()
        cur.close()
        conn.close()
        
        if not subscription:
            return False
        
        now = datetime.now()
        
        if subscription['plan_type'] == 'trial':
            if subscription['trial_ends_at'] and now < subscription['trial_ends_at']:
                return True
        elif subscription['plan_type'] == 'monthly':
            if subscription['subscription_ends_at'] and now < subscription['subscription_ends_at']:
                return True
        
        return False
    except Exception:
        return False

def lemmatize_phrase(phrase: str) -> str:
    '''–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ñ—Ä–∞–∑—ã ‚Äî –ø—Ä–∏–≤–æ–¥–∏—Ç —Å–ª–æ–≤–∞ –∫ –Ω–∞—á–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ'''
    words = phrase.lower().split()
    lemmas = []
    for word in words:
        parsed = morph.parse(word)[0]
        lemmas.append(parsed.normal_form)
    return ' '.join(lemmas)

def detect_intent(phrase: str) -> str:
    '''–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π –∏–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π intent'''
    commercial_markers = [
        '–∫—É–ø–∏—Ç—å', '—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–∑–∞–∫–∞–∑–∞—Ç—å', '–¥–æ—Å—Ç–∞–≤–∫–∞', 
        '–Ω–µ–¥–æ—Ä–æ–≥–æ', '–¥–µ—à–µ–≤–æ', '–º–∞–≥–∞–∑–∏–Ω', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç', '—Å–∞–π—Ç',
        '–ø—Ä–æ–¥–∞–∂–∞', '—Å–∫–∏–¥–∫–∞', '–∞–∫—Ü–∏—è', '–æ—Ñ–∏—Å', '—Ç–µ–ª–µ—Ñ–æ–Ω'
    ]
    
    info_markers = [
        '–∫–∞–∫', '—á—Ç–æ', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∫–∞–∫–æ–π',
        '–æ—Ç–∑—ã–≤—ã', '—Ä–µ–π—Ç–∏–Ω–≥', '–ª—É—á—à–∏–π', '—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ', '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è',
        '—Ñ–æ—Ç–æ', '–≤–∏–¥–µ–æ', '—Å—Ç–∞—Ç—å—è', '—Ñ–æ—Ä—É–º'
    ]
    
    phrase_lower = phrase.lower()
    
    commercial_score = sum(1 for marker in commercial_markers if marker in phrase_lower)
    info_score = sum(1 for marker in info_markers if marker in phrase_lower)
    
    if commercial_score > info_score:
        return 'commercial'
    elif info_score > commercial_score:
        return 'informational'
    else:
        return 'general'

def detect_minus_words(phrases: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
    '''
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–∏–Ω—É—Å-—Å–ª–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π —Ä–µ–∫–ª–∞–º—ã
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –Ω–µ—Ü–µ–ª–µ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    '''
    minus_categories = {
        'free': {
            'name': 'üÜì –ë–µ—Å–ø–ª–∞—Ç–Ω–æ / –•–∞–ª—è–≤–∞',
            'keywords': ['–±–µ—Å–ø–ª–∞—Ç–Ω–æ', '–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π', '–¥–∞—Ä–æ–º', '–±–µ–∑–≤–æ–∑–º–µ–∑–¥–Ω–æ', '–∑–∞–¥–∞—Ä–º–∞', 'free'],
            'phrases': []
        },
        'diy': {
            'name': 'üîß –°–≤–æ–∏–º–∏ —Ä—É–∫–∞–º–∏ / DIY',
            'keywords': ['—Å–≤–æ–∏–º–∏ —Ä—É–∫–∞–º–∏', '—Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ', '—Å–∞–º', '—Å–∞–º–æ–º—É', 'diy', '–∫–∞–∫ —Å–¥–µ–ª–∞—Ç—å', '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è'],
            'phrases': []
        },
        'competitors': {
            'name': 'üè¢ –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã / –ü–ª–æ—â–∞–¥–∫–∏',
            'keywords': ['–∞–≤–∏—Ç–æ', '—Ü–∏–∞–Ω', '–¥–æ–º–∫–ª–∏–∫', '—è–Ω–¥–µ–∫—Å –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å', '—é–ª–∞', '–∏–∑ —Ä—É–∫ –≤ —Ä—É–∫–∏'],
            'phrases': []
        },
        'info': {
            'name': '‚ÑπÔ∏è –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã',
            'keywords': ['—á—Ç–æ —Ç–∞–∫–æ–µ', '–∫–∞–∫ –≤—ã–±—Ä–∞—Ç—å', '–∫–∞–∫–æ–π –ª—É—á—à–µ', '–æ—Ç–ª–∏—á–∏—è', '—Ä–∞–∑–Ω–∏—Ü–∞', '–ø–ª—é—Å—ã –º–∏–Ω—É—Å—ã', '—Å–æ–≤–µ—Ç—ã'],
            'phrases': []
        },
        'job': {
            'name': 'üíº –†–∞–±–æ—Ç–∞ / –í–∞–∫–∞–Ω—Å–∏–∏',
            'keywords': ['–≤–∞–∫–∞–Ω—Å–∏–∏', '—Ä–∞–±–æ—Ç–∞', '—Ä–µ–∑—é–º–µ', '–∑–∞—Ä–ø–ª–∞—Ç–∞', '—Ç—Ä–µ–±—É—é—Ç—Å—è', '–∏—â—É —Ä–∞–±–æ—Ç—É', '–∫–∞—Ä—å–µ—Ä–∞'],
            'phrases': []
        },
        'education': {
            'name': 'üéì –û–±—É—á–µ–Ω–∏–µ / –ö—É—Ä—Å—ã',
            'keywords': ['–∫—É—Ä—Å—ã', '–æ–±—É—á–µ–Ω–∏–µ', '—Å–µ–º–∏–Ω–∞—Ä', '—Ç—Ä–µ–Ω–∏–Ω–≥', '–≤–µ–±–∏–Ω–∞—Ä', '–º–∞—Å—Ç–µ—Ä –∫–ª–∞—Å—Å', '—É—Ä–æ–∫–∏'],
            'phrases': []
        },
        'download': {
            'name': 'üì• –°–∫–∞—á–∞—Ç—å / –ó–∞–≥—Ä—É–∑–∏—Ç—å',
            'keywords': ['—Å–∫–∞—á–∞—Ç—å', '–∑–∞–≥—Ä—É–∑–∏—Ç—å', 'download', '—Ç–æ—Ä—Ä–µ–Ω—Ç', '–æ–Ω–ª–∞–π–Ω', '—Å–º–æ—Ç—Ä–µ—Ç—å'],
            'phrases': []
        },
        'porn': {
            'name': 'üîû –í–∑—Ä–æ—Å–ª—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç',
            'keywords': ['–ø–æ—Ä–Ω–æ', '—Å–µ–∫—Å', 'xxx', '—ç—Ä–æ—Ç–∏–∫–∞', '–∏–Ω—Ç–∏–º'],
            'phrases': []
        },
        'other': {
            'name': '‚ùì –ü—Ä–æ—á–∏–µ –Ω–µ—Ü–µ–ª–µ–≤—ã–µ',
            'keywords': ['–∏–≥—Ä–∞', '–∏–≥—Ä—ã', '–º—É–ª—å—Ç—Ñ–∏–ª—å–º', '–∫–∞—Ä—Ç–∏–Ω–∫–∏', '—Ä–∏—Å—É–Ω–æ–∫', '—Ä–∞—Å–∫—Ä–∞—Å–∫–∞', '—à—É—Ç–∫–∏', '–∞–Ω–µ–∫–¥–æ—Ç—ã'],
            'phrases': []
        }
    }
    
    for phrase_data in phrases:
        phrase_lower = phrase_data['phrase'].lower()
        
        matched = False
        for category_key, category_data in minus_categories.items():
            for keyword in category_data['keywords']:
                if keyword in phrase_lower:
                    category_data['phrases'].append(phrase_data)
                    matched = True
                    break
            if matched:
                break
    
    result = {}
    for key, data in minus_categories.items():
        if len(data['phrases']) > 0:
            result[key] = {
                'name': data['name'],
                'count': len(data['phrases']),
                'total_volume': sum(p['count'] for p in data['phrases']),
                'phrases': sorted(data['phrases'], key=lambda x: x['count'], reverse=True)
            }
    
    return result

def calculate_tfidf(phrases: List[str]) -> List[Dict[str, float]]:
    '''–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è TF-IDF –±–µ–∑ scikit-learn'''
    stop_words = {
        '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–∏–∑', '–∏', '–∏–ª–∏', '–∫–∞–∫', '—á—Ç–æ', '–∑–∞',
        '—ç—Ç–æ', '—Ç–æ', '—Ç–∞–∫', '–Ω–æ', '–∞', '–æ', '—É', '–æ—Ç', '–∫', '–¥–æ', '–ø—Ä–∏',
        '–±–µ–∑', '–ø–æ–¥', '–Ω–∞–¥', '–º–µ–∂–¥—É', '–ø–µ—Ä–µ–¥', '—á–µ—Ä–µ–∑', '–ø–æ—Å–ª–µ'
    }
    
    doc_words = []
    for phrase in phrases:
        words = [w for w in phrase.lower().split() if w not in stop_words and len(w) > 2]
        doc_words.append(words)
    
    word_doc_count = defaultdict(int)
    for words in doc_words:
        unique_words = set(words)
        for word in unique_words:
            word_doc_count[word] += 1
    
    n_docs = len(phrases)
    idf = {}
    for word, count in word_doc_count.items():
        idf[word] = math.log(n_docs / count)
    
    tfidf_vectors = []
    for words in doc_words:
        word_count = defaultdict(int)
        for word in words:
            word_count[word] += 1
        
        tf = {w: count / len(words) if len(words) > 0 else 0 for w, count in word_count.items()}
        
        tfidf = {w: tf[w] * idf.get(w, 0) for w in tf}
        tfidf_vectors.append(tfidf)
    
    return tfidf_vectors

def cosine_similarity_simple(vec1: Dict[str, float], vec2: Dict[str, float]) -> float:
    '''–ö–æ—Å–∏–Ω—É—Å–Ω–∞—è –±–ª–∏–∑–æ—Å—Ç—å –º–µ–∂–¥—É –¥–≤—É–º—è –≤–µ–∫—Ç–æ—Ä–∞–º–∏'''
    all_words = set(vec1.keys()) | set(vec2.keys())
    
    dot_product = sum(vec1.get(w, 0) * vec2.get(w, 0) for w in all_words)
    
    norm1 = math.sqrt(sum(v**2 for v in vec1.values()))
    norm2 = math.sqrt(sum(v**2 for v in vec2.values()))
    
    if norm1 == 0 or norm2 == 0:
        return 0.0
    
    return dot_product / (norm1 * norm2)

def clusterize_advanced(phrases: List[Dict[str, Any]], mode: str = 'context', region_names: List[str] = None, selected_intents: List[str] = None) -> tuple:
    '''
    –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ —É–ª—É—á—à–µ–Ω–Ω—ã–π TF-IDF –∞–ª–≥–æ—Ä–∏—Ç–º
    –°–æ–∑–¥–∞—ë—Ç –ú–ù–û–ì–û –º–∞–ª–µ–Ω—å–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ –æ–¥–Ω–æ–≥–æ –±–æ–ª—å—à–æ–≥–æ
    Args:
        phrases: —Å–ø–∏—Å–æ–∫ —Ñ—Ä–∞–∑ —Å —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å—é
        mode: 'context' (–Ø–Ω–¥–µ–∫—Å.–î–∏—Ä–µ–∫—Ç) –∏–ª–∏ 'seo' (SEO)
    Returns: (clusters, minus_words)
    '''
    if len(phrases) < 5:
        clusters = smart_clusterize(phrases, mode)
        minus_words = detect_minus_words(phrases) if mode == 'context' else {}
        return clusters, minus_words
    
    print(f'[ADVANCED] Starting advanced clustering for {len(phrases)} phrases, mode: {mode}')
    
    tfidf_vectors = calculate_tfidf([p['phrase'] for p in phrases])
    
    if mode == 'context':
        similarity_threshold = 0.15
        min_cluster_size = 2
    else:
        similarity_threshold = 0.2
        min_cluster_size = 3
    
    used = set()
    clusters_dict = []
    
    for i, phrase_a in enumerate(phrases):
        if i in used:
            continue
        
        cluster = [i]
        used.add(i)
        
        for j, phrase_b in enumerate(phrases):
            if j in used or j == i:
                continue
            
            similarity = cosine_similarity_simple(tfidf_vectors[i], tfidf_vectors[j])
            
            if similarity >= similarity_threshold:
                cluster.append(j)
                used.add(j)
        
        if len(cluster) >= min_cluster_size:
            clusters_dict.append([phrases[idx] for idx in cluster])
    
    remaining = [phrases[i] for i in range(len(phrases)) if i not in used]
    if remaining:
        print(f'[ADVANCED] {len(remaining)} phrases remain unclustered')
        for phrase in remaining:
            clusters_dict.append([phrase])
    
    clusters = []
    for cluster_phrases in clusters_dict:
        cluster_name = generate_cluster_name(cluster_phrases)
        total_volume = sum(p['count'] for p in cluster_phrases)
        
        clusters.append({
            'name': cluster_name,
            'phrases': sorted(cluster_phrases, key=lambda x: x['count'], reverse=True),
            'count': len(cluster_phrases),
            'total_volume': total_volume
        })
    
    clusters.sort(key=lambda x: x['total_volume'], reverse=True)
    
    minus_words = detect_minus_words(phrases) if mode == 'context' else {}
    
    print(f'[ADVANCED] Created {len(clusters)} clusters (avg size: {len(phrases) / len(clusters):.1f})')
    return clusters, minus_words

def generate_cluster_name(cluster_phrases: List[Dict[str, Any]]) -> str:
    '''–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—â–∏—Ö —Å–ª–æ–≤'''
    stop_words = {
        '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–∏–∑', '–∏', '–∏–ª–∏', '–∫–∞–∫', '—á—Ç–æ', '–∑–∞',
        '—ç—Ç–æ', '—Ç–æ', '—Ç–∞–∫', '–Ω–æ', '–∞', '–æ', '—É', '–æ—Ç', '–∫', '–¥–æ', '–ø—Ä–∏'
    }
    
    word_counts = defaultdict(int)
    for p in cluster_phrases:
        words = [w.lower() for w in p['phrase'].split() if w.lower() not in stop_words and len(w) > 2]
        for word in words:
            word_counts[word] += 1
    
    if not word_counts:
        return 'üìÇ –ö–ª–∞—Å—Ç–µ—Ä'
    
    top_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:3]
    name = ' '.join([w[0] for w in top_words])
    
    return f'üîπ {name.capitalize()}'

def clusterize_with_openai(phrases: List[Dict[str, Any]], mode: str = 'context', region_names: List[str] = None, selected_intents: List[str] = None) -> tuple:
    '''
    –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ OpenAI GPT-4o —Å —É—á—ë—Ç–æ–º —Ä–µ–≥–∏–æ–Ω–æ–≤ –∏ –∏–Ω—Ç–µ–Ω—Ç–æ–≤
    Args:
        phrases: —Å–ø–∏—Å–æ–∫ —Ñ—Ä–∞–∑ —Å —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å—é
        mode: 'context' (–Ø–Ω–¥–µ–∫—Å.–î–∏—Ä–µ–∫—Ç) –∏–ª–∏ 'seo' (SEO)
        region_names: —Å–ø–∏—Å–æ–∫ —Ä–µ–≥–∏–æ–Ω–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä ['–ú–æ—Å–∫–≤–∞', '–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥'])
        selected_intents: –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –∏–Ω—Ç–µ–Ω—Ç—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä ['commercial', 'transactional'])
    Returns: (clusters, minus_words)
    '''
    openai_key = os.environ.get('OPENAI_API_KEY')
    if not openai_key:
        print('[OPENAI] API key not found - using TF-IDF clustering')
        clusters = smart_clusterize(phrases, mode)
        minus_words = detect_minus_words(phrases) if mode == 'context' else {}
        return clusters, minus_words
    
    phrases_text = '\n'.join([f"{p['phrase']} ({p['count']} –ø–æ–∫–∞–∑–æ–≤)" for p in phrases[:150]])
    
    regions_text = ', '.join(region_names) if region_names else '–†–æ—Å—Å–∏—è'
    
    intent_descriptions = {
        'commercial': '–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ (–∫—É–ø–∏—Ç—å, —Ü–µ–Ω–∞, –∑–∞–∫–∞–∑–∞—Ç—å)',
        'transactional': '–¢—Ä–∞–Ω–∑–∞–∫—Ü–∏–æ–Ω–Ω—ã–µ (–æ—Ñ–æ—Ä–º–∏—Ç—å, –∑–∞–ø–∏—Å–∞—Ç—å—Å—è, –ø–æ–ª—É—á–∏—Ç—å)',
        'informational': '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ (–∫–∞–∫, —á—Ç–æ —Ç–∞–∫–æ–µ, –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è)',
        'navigational': '–ù–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–µ (—Å–∞–π—Ç, –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π, –∫–æ–Ω—Ç–∞–∫—Ç—ã)'
    }
    
    selected_intents_text = ', '.join([intent_descriptions.get(i, i) for i in selected_intents]) if selected_intents else '–í—Å–µ —Ç–∏–ø—ã'
    
    if mode == 'context':
        prompt = f"""–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è –Ø–Ω–¥–µ–∫—Å.–î–∏—Ä–µ–∫—Ç.

üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û - –ß–ò–¢–ê–ô –í–ù–ò–ú–ê–¢–ï–õ–¨–ù–û:

–¢—ã –û–ë–Ø–ó–ê–ù –≤—ã–ø–æ–ª–Ω–∏—Ç—å —ç—Ç–∏ –ø—Ä–∞–≤–∏–ª–∞, –∏–Ω–∞—á–µ —Ç–≤–æ–π –æ—Ç–≤–µ—Ç –±—É–¥–µ—Ç –æ—Ç–∫–ª–æ–Ω—ë–Ω:

1Ô∏è‚É£ –ö–û–ü–ò–†–£–ô –§–†–ê–ó–´ –°–ò–ú–í–û–õ-–í-–°–ò–ú–í–û–õ
   ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û: "–∫—É–ø–∏—Ç—å –∫–≤–∞—Ä—Ç–∏—Ä—É –º–æ—Å–∫–≤–∞" ‚Üí "–∫—É–ø–∏—Ç—å –∫–≤–∞—Ä—Ç–∏—Ä—É –º–æ—Å–∫–≤–∞"
   ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û: "–∫—É–ø–∏—Ç—å –∫–≤–∞—Ä—Ç–∏—Ä—É –º–æ—Å–∫–≤–∞" ‚Üí "–∫—É–ø–∏—Ç—å –∫–≤–∞—Ä—Ç–∏—Ä—É –≤ –º–æ—Å–∫–≤–µ"
   ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û: "–∫—É–ø–∏—Ç—å –∫–≤–∞—Ä—Ç–∏—Ä—É –º–æ—Å–∫–≤–∞" ‚Üí "–∫—É–ø–∏—Ç—å –∫–≤–∞—Ä—Ç–∏—Ä—É"

2Ô∏è‚É£ –ù–ï –î–û–ë–ê–í–õ–Ø–ô –§–†–ê–ó–´ –û–¢ –°–ï–ë–Ø
   ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ—à—å –¢–û–õ–¨–ö–û —Ñ—Ä–∞–∑—ã –∏–∑ —Å–ø–∏—Å–∫–∞ –Ω–∏–∂–µ
   ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û: –î–æ–±–∞–≤–∏–ª "–∫—É–ø–∏—Ç—å 1 –∫–≤–∞—Ä—Ç–∏—Ä–∞ –≤—Ç–æ—Ä–∏—á–∫–∞" (—ç—Ç–æ–≥–æ –Ω–µ –±—ã–ª–æ –≤ —Å–ø–∏—Å–∫–µ)
   
3Ô∏è‚É£ –ù–ï –£–î–ê–õ–Ø–ô –§–†–ê–ó–´
   ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û: –í—Å–µ 50 —Ñ—Ä–∞–∑ –∏–∑ —Å–ø–∏—Å–∫–∞ –¥–æ–ª–∂–Ω—ã –ø–æ–ø–∞—Å—Ç—å –≤ –∫–ª–∞—Å—Ç–µ—Ä—ã
   ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û: –ü—Ä–æ–ø—É—Å—Ç–∏–ª "–∫–≤–∞—Ä—Ç–∏—Ä—ã –≤ —Å—Ç–∞–≤—Ä–æ–ø–æ–ª–µ –∫—É–ø–∏—Ç—å 2" –ø–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω–∞ —Å—Ç—Ä–∞–Ω–Ω–∞—è

4Ô∏è‚É£ –ö–ê–ñ–î–ê–Ø —Ñ—Ä–∞–∑–∞ –¥–æ–ª–∂–Ω–∞ –ø–æ–ø–∞—Å—Ç—å –≤ –û–î–ò–ù –∫–ª–∞—Å—Ç–µ—Ä

üéØ –ú–ï–¢–û–î–û–õ–û–ì–ò–Ø –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–ò (–Ø–Ω–¥–µ–∫—Å.–î–∏—Ä–µ–∫—Ç):

**–ü—Ä–∏–Ω—Ü–∏–ø 1: –û–¥–∏–Ω –ø–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω—Ç–µ–Ω—Ç = –û–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä**
–ì—Ä—É–ø–ø–∏—Ä—É–π —Ñ—Ä–∞–∑—ã —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º –Ω–∞–º–µ—Ä–µ–Ω–∏–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:
- "–∫—É–ø–∏—Ç—å –∫–≤–∞—Ä—Ç–∏—Ä—É –º–æ—Å–∫–≤–∞" + "–∫–≤–∞—Ä—Ç–∏—Ä—ã –∫—É–ø–∏—Ç—å" ‚Üí –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä (–ø–æ–∫—É–ø–∫–∞)
- "—Å–Ω—è—Ç—å –∫–≤–∞—Ä—Ç–∏—Ä—É" ‚Üí –æ—Ç–¥–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä (–∞—Ä–µ–Ω–¥–∞)
- "–∏–ø–æ—Ç–µ–∫–∞ –Ω–∞ –∫–≤–∞—Ä—Ç–∏—Ä—É" ‚Üí –æ—Ç–¥–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä (—Ñ–∏–Ω–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–∏–µ)

**–ü—Ä–∏–Ω—Ü–∏–ø 2: –û–¥–∏–Ω –æ–±—ä–µ–∫—Ç/—É—Å–ª—É–≥–∞ = –û–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä**
–†–∞–∑–¥–µ–ª—è–π –ø–æ —Ç–∏–ø—É —Ç–æ–≤–∞—Ä–∞/—É—Å–ª—É–≥–∏:
- "–æ–¥–Ω–æ–∫–æ–º–Ω–∞—Ç–Ω–∞—è –∫–≤–∞—Ä—Ç–∏—Ä–∞" ‚â† "–¥–≤—É—Ö–∫–æ–º–Ω–∞—Ç–Ω–∞—è –∫–≤–∞—Ä—Ç–∏—Ä–∞" (—Ä–∞–∑–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã)
- "–ø–ª–∞—Å—Ç–∏–∫–æ–≤—ã–µ –æ–∫–Ω–∞" ‚â† "–¥–µ—Ä–µ–≤—è–Ω–Ω—ã–µ –æ–∫–Ω–∞" (—Ä–∞–∑–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã)
- "–¥–µ—Ç—Å–∫–∞—è –æ–±—É–≤—å" ‚â† "–º—É–∂—Å–∫–∞—è –æ–±—É–≤—å" (—Ä–∞–∑–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã)

**–ü—Ä–∏–Ω—Ü–∏–ø 3: –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Ä–∞–∑–¥–µ–ª—è—é—Ç –∫–ª–∞—Å—Ç–µ—Ä—ã**
–ï—Å–ª–∏ —Ñ—Ä–∞–∑—ã –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –ø–æ:
- –î–µ–π—Å—Ç–≤–∏—é: –∫—É–ø–∏—Ç—å/–∑–∞–∫–∞–∑–∞—Ç—å/—É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å/–æ—Ç—Ä–µ–º–æ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å
- –¶–µ–Ω–µ: –Ω–µ–¥–æ—Ä–æ–≥–æ/–ø—Ä–µ–º–∏—É–º/—Å–æ —Å–∫–∏–¥–∫–æ–π/–¥–µ—à–µ–≤–æ
- –°—Ä–æ—á–Ω–æ—Å—Ç–∏: —Å—Ä–æ—á–Ω–æ/–±—ã—Å—Ç—Ä–æ/–∫—Ä—É–≥–ª–æ—Å—É—Ç–æ—á–Ω–æ
- –ë—Ä–µ–Ω–¥—É/–º–∞—Ç–µ—Ä–∏–∞–ª—É/—Ä–∞–∑–º–µ—Ä—É
‚Üí —Å–æ–∑–¥–∞–≤–∞–π –û–¢–î–ï–õ–¨–ù–´–ï –∫–ª–∞—Å—Ç–µ—Ä—ã

**–ü—Ä–∏–Ω—Ü–∏–ø 4: –†–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞ 3-12 —Ñ—Ä–∞–∑**
- –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π –∫–ª–∞—Å—Ç–µ—Ä (>15) ‚Üí —Ä–∞–∑–¥—Ä–æ–±–∏ –Ω–∞ –ø–æ–¥–≥—Ä—É–ø–ø—ã
- –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π (<3) ‚Üí –æ–±—ä–µ–¥–∏–Ω–∏ —Å–æ —Å—Ö–æ–∂–∏–º –∏–Ω—Ç–µ–Ω—Ç–æ–º

üö® –í–ê–ñ–ù–û: –ù–ï —Å–æ–∑–¥–∞–≤–∞–π –æ–¥–∏–Ω –≥–∏–≥–∞–Ω—Ç—Å–∫–∏–π –∫–ª–∞—Å—Ç–µ—Ä!
–ï—Å–ª–∏ –≤–∏–¥–∏—à—å —Ä–∞–∑–Ω—ã–µ –∏–Ω—Ç–µ–Ω—Ç—ã/–¥–µ–π—Å—Ç–≤–∏—è/–æ–±—ä–µ–∫—Ç—ã ‚Äî –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —Ä–∞–∑–¥–µ–ª—è–π –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã.
–ú–ò–ù–ò–ú–£–ú 3-5 –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è {len(phrases[:150])} —Ñ—Ä–∞–∑.

**–ü—Ä–∏–Ω—Ü–∏–ø 5: –ù–∞–∑–≤–∞–Ω–∏–µ = —Å–º—ã—Å–ª –≥—Ä—É–ø–ø—ã**
–ù–∞–∑–≤–∞–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –æ—Ç—Ä–∞–∂–∞—Ç—å –û–ë–©–ò–ô –∏–Ω—Ç–µ–Ω—Ç:
‚úÖ "–ö—É–ø–∏—Ç—å –∫–≤–∞—Ä—Ç–∏—Ä—É –ú–æ—Å–∫–≤–∞"
‚úÖ "–†–µ–º–æ–Ω—Ç —Ö–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫–æ–≤ –Ω–∞ –¥–æ–º—É"
‚úÖ "–î–µ—Ç—Å–∫–∞—è –æ–±—É–≤—å –Ω–µ–¥–æ—Ä–æ–≥–æ"
‚ùå "–ö–≤–∞—Ä—Ç–∏—Ä—ã" (—Å–ª–∏—à–∫–æ–º —à–∏—Ä–æ–∫–æ)
‚ùå "–∫—É–ø–∏—Ç—å" (–Ω–µ –ø–æ–Ω—è—Ç–Ω–æ —á—Ç–æ)

üìä –ö–û–ù–¢–ï–ö–°–¢ –ó–ê–î–ê–ß–ò:
–†–µ–≥–∏–æ–Ω—ã: {regions_text}
–ò–Ω—Ç–µ–Ω—Ç—ã: {selected_intents_text}

üìã –§–†–ê–ó–´ –î–õ–Ø –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–ò ({len(phrases[:150])} —à—Ç):
{phrases_text}

‚ö° –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (JSON):
{{
  "clusters": [
    {{
      "cluster_name": "–ö—Ä–∞—Ç–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–Ω—Ç–µ–Ω—Ç–∞",
      "intent": "commercial",
      "phrases": [
        {{"phrase": "–¢–û–ß–ù–ê–Ø –ö–û–ü–ò–Ø –ò–ó –°–ü–ò–°–ö–ê –í–´–®–ï (–∫–æ–ø–∏—Ä—É–π —á–µ—Ä–µ–∑ Ctrl+C, Ctrl+V)", "count": —á–∏—Å–ª–æ_–∏–∑_—Å–ø–∏—Å–∫–∞}}
      ]
    }}
  ],
  "minus_words": {{}}
}}

‚úã –°–¢–û–ü! –ü–ï–†–ï–î –û–¢–ü–†–ê–í–ö–û–ô –û–¢–í–ï–¢–ê –ü–†–û–í–ï–†–¨:
1. –ü–æ–¥—Å—á–∏—Ç–∞–π —Ñ—Ä–∞–∑—ã: –≤ —Ç–≤–æ—ë–º JSON –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –†–û–í–ù–û {len(phrases[:150])} —Ñ—Ä–∞–∑ (–∫–∞–∫ –≤ —Å–ø–∏—Å–∫–µ –≤—ã—à–µ)
2. –û—Ç–∫—Ä–æ–π —Å–ø–∏—Å–æ–∫ –≤—ã—à–µ –∏ —Å—Ä–∞–≤–Ω–∏ –ø–µ—Ä–≤—ã–µ 3 —Ñ—Ä–∞–∑—ã ‚Äî –æ–Ω–∏ –ò–î–ï–ù–¢–ò–ß–ù–´?
3. –¢—ã –ù–ï –¥–æ–±–∞–≤–∏–ª –Ω–∏ –æ–¥–Ω–æ–π —Ñ—Ä–∞–∑—ã –æ—Ç —Å–µ–±—è?
4. –¢—ã –ù–ï –ø—Ä–æ–ø—É—Å—Ç–∏–ª –Ω–∏ –æ–¥–Ω–æ–π —Ñ—Ä–∞–∑—ã –∏–∑ —Å–ø–∏—Å–∫–∞?
5. –í—Å–µ —Ñ—Ä–∞–∑—ã —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω—ã –°–ò–ú–í–û–õ-–í-–°–ò–ú–í–û–õ –≤–∫–ª—é—á–∞—è –ø—Ä–æ–±–µ–ª—ã –∏ —Ü–∏—Ñ—Ä—ã?

–ï—Å–ª–∏ —Ö–æ—Ç—å –Ω–∞ –æ–¥–∏–Ω –≤–æ–ø—Ä–æ—Å –æ—Ç–≤–µ—Ç "–ù–ï–¢" ‚Äî –ò–°–ü–†–ê–í–¨ –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π!"""
    else:
        prompt = f"""–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ SEO-–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–∞–π—Ç–∞.

üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û - –ß–ò–¢–ê–ô –í–ù–ò–ú–ê–¢–ï–õ–¨–ù–û:

–¢—ã –û–ë–Ø–ó–ê–ù –≤—ã–ø–æ–ª–Ω–∏—Ç—å —ç—Ç–∏ –ø—Ä–∞–≤–∏–ª–∞, –∏–Ω–∞—á–µ —Ç–≤–æ–π –æ—Ç–≤–µ—Ç –±—É–¥–µ—Ç –æ—Ç–∫–ª–æ–Ω—ë–Ω:

1Ô∏è‚É£ –ö–û–ü–ò–†–£–ô –§–†–ê–ó–´ –°–ò–ú–í–û–õ-–í-–°–ò–ú–í–û–õ
   ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û: "—Ä–µ–º–æ–Ω—Ç –∫–≤–∞—Ä—Ç–∏—Ä –º–æ—Å–∫–≤–∞" ‚Üí "—Ä–µ–º–æ–Ω—Ç –∫–≤–∞—Ä—Ç–∏—Ä –º–æ—Å–∫–≤–∞"
   ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û: "—Ä–µ–º–æ–Ω—Ç –∫–≤–∞—Ä—Ç–∏—Ä –º–æ—Å–∫–≤–∞" ‚Üí "—Ä–µ–º–æ–Ω—Ç –∫–≤–∞—Ä—Ç–∏—Ä –≤ –º–æ—Å–∫–≤–µ"

2Ô∏è‚É£ –ù–ï –î–û–ë–ê–í–õ–Ø–ô –§–†–ê–ó–´ –û–¢ –°–ï–ë–Ø
   ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û: –¢–æ–ª—å–∫–æ —Ñ—Ä–∞–∑—ã –∏–∑ —Å–ø–∏—Å–∫–∞ –Ω–∏–∂–µ
   ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û: –î–æ–±–∞–≤–∏–ª —Å–≤–æ–∏ –≤–∞—Ä–∏–∞–Ω—Ç—ã
   
3Ô∏è‚É£ –ù–ï –£–î–ê–õ–Ø–ô –§–†–ê–ó–´
   ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û: –í—Å–µ —Ñ—Ä–∞–∑—ã –ø–æ–ø–∞–¥–∞—é—Ç –≤ –∫–ª–∞—Å—Ç–µ—Ä—ã
   ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û: –ü—Ä–æ–ø—É—Å—Ç–∏–ª "—Å—Ç—Ä–∞–Ω–Ω—ã–µ" —Ñ—Ä–∞–∑—ã

4Ô∏è‚É£ –ö–ê–ñ–î–ê–Ø —Ñ—Ä–∞–∑–∞ = –û–î–ò–ù –∫–ª–∞—Å—Ç–µ—Ä

üéØ –ú–ï–¢–û–î–û–õ–û–ì–ò–Ø SEO-–ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–ò:

**–ü—Ä–∏–Ω—Ü–∏–ø 1: –û–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä = –û–¥–Ω–∞ –ø–æ—Å–∞–¥–æ—á–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞**
–ì—Ä—É–ø–ø–∏—Ä—É–π —Ñ—Ä–∞–∑—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –∑–∞–∫—Ä—ã—Ç—å –û–î–ù–û–ô —Å—Ç—Ä–∞–Ω–∏—Ü–µ–π:
- "–∫—É–ø–∏—Ç—å –¥–∏–≤–∞–Ω" + "–¥–∏–≤–∞–Ω —Ü–µ–Ω–∞" + "–∑–∞–∫–∞–∑–∞—Ç—å –¥–∏–≤–∞–Ω" ‚Üí –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä (–∫–∞—Ç–∞–ª–æ–≥ –¥–∏–≤–∞–Ω–æ–≤)
- "—É–≥–ª–æ–≤–æ–π –¥–∏–≤–∞–Ω" + "–º–æ–¥—É–ª—å–Ω—ã–π –¥–∏–≤–∞–Ω" ‚Üí –æ—Ç–¥–µ–ª—å–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã (—Ä–∞–∑–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏)

**–ü—Ä–∏–Ω—Ü–∏–ø 2: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –±–ª–∏–∑–æ—Å—Ç—å**
–û–±—ä–µ–¥–∏–Ω—è–π —Å–∏–Ω–æ–Ω–∏–º—ã –∏ –±–ª–∏–∑–∫–∏–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏:
‚úÖ "—Ä–µ–º–æ–Ω—Ç –∫–≤–∞—Ä—Ç–∏—Ä" + "—Ä–µ–º–æ–Ω—Ç –∫–≤–∞—Ä—Ç–∏—Ä—ã" + "–∫–≤–∞—Ä—Ç–∏—Ä–Ω—ã–π —Ä–µ–º–æ–Ω—Ç" ‚Üí –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä
‚úÖ "–¥–æ—Å—Ç–∞–≤–∫–∞ –ø–∏—Ü—Ü—ã" + "–ø–∏—Ü—Ü–∞ —Å –¥–æ—Å—Ç–∞–≤–∫–æ–π" + "–∑–∞–∫–∞–∑–∞—Ç—å –ø–∏—Ü—Ü—É" ‚Üí –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä
‚ùå "—Ä–µ–º–æ–Ω—Ç –∫–≤–∞—Ä—Ç–∏—Ä" + "–¥–∏–∑–∞–π–Ω –∏–Ω—Ç–µ—Ä—å–µ—Ä–∞" ‚Üí —Ä–∞–∑–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã (—Ä–∞–∑–Ω—ã–µ —É—Å–ª—É–≥–∏)

**–ü—Ä–∏–Ω—Ü–∏–ø 3: –£—á–∏—Ç—ã–≤–∞–π –∏–Ω—Ç–µ–Ω—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è**
–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –∏ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ ‚Äî –≤ —Ä–∞–∑–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã:
- "–∫–∞–∫ –≤—ã–±—Ä–∞—Ç—å –Ω–æ—É—Ç–±—É–∫" (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è) ‚â† "–∫—É–ø–∏—Ç—å –Ω–æ—É—Ç–±—É–∫" (–ø–æ–∫—É–ø–∫–∞)
- "—Ä–µ—Ü–µ–ø—Ç –±–æ—Ä—â–∞" (–∏–Ω—Ñ–æ) ‚â† "–±–æ—Ä—â –¥–æ—Å—Ç–∞–≤–∫–∞" (–∑–∞–∫–∞–∑)

**–ü—Ä–∏–Ω—Ü–∏–ø 4: –†–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞ 8-30 —Ñ—Ä–∞–∑**
- –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Ç–æ–≤–∞—Ä–æ–≤: 15-30 —Ñ—Ä–∞–∑
- –î–ª—è —É—Å–ª—É–≥: 8-20 —Ñ—Ä–∞–∑
- –î–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü: 10-25 —Ñ—Ä–∞–∑

**–ü—Ä–∏–Ω—Ü–∏–ø 5: –†–µ–≥–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å**
–§—Ä–∞–∑—ã —Å —Ä–∞–∑–Ω—ã–º–∏ –≥–æ—Ä–æ–¥–∞–º–∏ ‚Äî –≤ –û–î–ò–ù –∫–ª–∞—Å—Ç–µ—Ä, –µ—Å–ª–∏ —É—Å–ª—É–≥–∞ –æ–¥–∏–Ω–∞–∫–æ–≤–∞—è:
‚úÖ "—Ä–µ–º–æ–Ω—Ç –º–æ—Å–∫–≤–∞" + "—Ä–µ–º–æ–Ω—Ç —Å–ø–±" ‚Üí –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä "–†–µ–º–æ–Ω—Ç –∫–≤–∞—Ä—Ç–∏—Ä"

**–ü—Ä–∏–Ω—Ü–∏–ø 6: –ù–∞–∑–≤–∞–Ω–∏–µ = —Ç–µ–º–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã**
‚úÖ "–ö—É–ø–∏—Ç—å –ø–ª–∞—Å—Ç–∏–∫–æ–≤—ã–µ –æ–∫–Ω–∞"
‚úÖ "–†–µ–º–æ–Ω—Ç —Å—Ç–∏—Ä–∞–ª—å–Ω—ã—Ö –º–∞—à–∏–Ω"
‚úÖ "–î–æ—Å—Ç–∞–≤–∫–∞ —Å—É—à–∏ –∏ —Ä–æ–ª–ª–æ–≤"
‚ùå "–û–∫–Ω–∞" (—Å–ª–∏—à–∫–æ–º –æ–±—â–µ–µ)
‚ùå "–£—Å–ª—É–≥–∏" (–Ω–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ)

üìä –ö–û–ù–¢–ï–ö–°–¢:
–†–µ–≥–∏–æ–Ω—ã: {regions_text}
–ò–Ω—Ç–µ–Ω—Ç—ã: {selected_intents_text}

üìã –§–†–ê–ó–´ ({len(phrases[:150])} —à—Ç):
{phrases_text}

‚ö° –û–¢–í–ï–¢ –í JSON:
{{
  "clusters": [
    {{
      "cluster_name": "–ü–æ–Ω—è—Ç–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ 2-5 —Å–ª–æ–≤",
      "intent": "commercial/transactional/informational/navigational",
      "phrases": [
        {{"phrase": "—Ç–æ—á–Ω–∞—è —Ñ—Ä–∞–∑–∞ –∏–∑ —Å–ø–∏—Å–∫–∞", "count": 1234}}
      ]
    }}
  ]
}}"""
    
    proxy_url = os.environ.get('OPENAI_PROXY_URL')
    proxies = {}
    if proxy_url:
        proxies = {
            'http': proxy_url,
            'https': proxy_url
        }
        print(f'[OPENAI] Using proxy: {proxy_url[:20]}...')
    
    try:
        response = requests.post(
            'https://api.openai.com/v1/chat/completions',
            headers={
                'Authorization': f'Bearer {openai_key}',
                'Content-Type': 'application/json'
            },
            json={
                'model': 'gpt-4o-mini',
                'messages': [
                    {'role': 'system', 'content': '–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤. –°—Ç—Ä–æ–≥–æ —Å–ª–µ–¥—É–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º. –ö–æ–ø–∏—Ä—É–π —Ñ—Ä–∞–∑—ã –¢–û–ß–ù–û –∫–∞–∫ –≤ —Å–ø–∏—Å–∫–µ. –û—Ç–≤–µ—á–∞–µ—à—å —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–º JSON.'},
                    {'role': 'user', 'content': prompt}
                ],
                'temperature': 0.0,
                'response_format': {'type': 'json_object'}
            },
            proxies=proxies if proxies else None,
            timeout=90
        )
        
        if response.status_code == 200:
            data = response.json()
            content = data['choices'][0]['message']['content']
            result = json.loads(content)
            clusters = result.get('clusters', [])
            minus_words = result.get('minus_words', {})
            
            # –í–ê–õ–ò–î–ê–¶–ò–Ø: –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ OpenAI –Ω–µ –∏–∑–º–µ–Ω–∏–ª —Ñ—Ä–∞–∑—ã
            original_phrases_set = {p['phrase'].strip().lower() for p in phrases}
            clustered_phrases_set = set()
            
            for cluster in clusters:
                for phrase_obj in cluster.get('phrases', []):
                    clustered_phrases_set.add(phrase_obj['phrase'].strip().lower())
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –Ω–µ—Ç –ª–∏—à–Ω–∏—Ö —Ñ—Ä–∞–∑ (–∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –±—ã–ª–æ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
            added_phrases = clustered_phrases_set - original_phrases_set
            if added_phrases:
                print(f'[OPENAI] ‚ùå ERROR: AI –¥–æ–±–∞–≤–∏–ª {len(added_phrases)} —Ñ—Ä–∞–∑ –æ—Ç —Å–µ–±—è! –û—Ç–∫–∞—Ç –∫ TF-IDF')
                print(f'[OPENAI] –ü—Ä–∏–º–µ—Ä—ã –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö: {list(added_phrases)[:3]}')
                clusters = smart_clusterize(phrases, mode)
                minus_words = detect_minus_words(phrases) if mode == 'context' else {}
                return clusters, minus_words
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ OpenAI –Ω–µ —É–¥–∞–ª–∏–ª —Ñ—Ä–∞–∑—ã
            deleted_phrases = original_phrases_set - clustered_phrases_set
            if deleted_phrases:
                print(f'[OPENAI] ‚ùå ERROR: AI —É–¥–∞–ª–∏–ª {len(deleted_phrases)} —Ñ—Ä–∞–∑! –û—Ç–∫–∞—Ç –∫ TF-IDF')
                print(f'[OPENAI] –ü—Ä–∏–º–µ—Ä—ã —É–¥–∞–ª—ë–Ω–Ω—ã—Ö: {list(deleted_phrases)[:5]}')
                clusters = smart_clusterize(phrases, mode)
                minus_words = detect_minus_words(phrases) if mode == 'context' else {}
                return clusters, minus_words
            
            print(f'[OPENAI] ‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è OK: {len(clusters)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, {len(clustered_phrases_set)}/{len(original_phrases_set)} —Ñ—Ä–∞–∑')
            if minus_words:
                total_minus = sum(len(v) for v in minus_words.values() if isinstance(v, list))
                print(f'[OPENAI] Detected {total_minus} minus-words')
            return clusters, minus_words
        else:
            print(f'[OPENAI] API error {response.status_code}: {response.text}, falling back to TF-IDF')
            clusters = smart_clusterize(phrases, mode)
            minus_words = detect_minus_words(phrases) if mode == 'context' else {}
            return clusters, minus_words
            
    except Exception as e:
        print(f'[OPENAI] Error: {str(e)}, falling back to TF-IDF')
        clusters = smart_clusterize(phrases, mode)
        minus_words = detect_minus_words(phrases) if mode == 'context' else {}
        return clusters, minus_words

def smart_clusterize(phrases: List[Dict[str, Any]], mode: str = 'seo') -> List[Dict[str, Any]]:
    '''
    –°—É–ø–µ—Ä-–ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å —Ä–∞–∑–Ω—ã–º–∏ —Ä–µ–∂–∏–º–∞–º–∏:
    - mode='seo': –®–∏—Ä–æ–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã (10-30 —Ñ—Ä–∞–∑) –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    - mode='context': –ë–∞–∑–æ–≤—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã (10-50 —Ñ—Ä–∞–∑) –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π —Ä–µ–∫–ª–∞–º—ã
    '''
    if not phrases:
        return []
    
    competitors = ['–∞–≤–∏—Ç–æ', '—Ü–∏–∞–Ω', '–¥–æ–º–∫–ª–∏–∫', '—è–Ω–¥–µ–∫—Å –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å', '—é–ª–∞', '–∏–∑ —Ä—É–∫ –≤ —Ä—É–∫–∏']
    competitor_phrases = []
    regular_phrases = []
    
    for p in phrases:
        is_competitor = any(comp in p['phrase'].lower() for comp in competitors)
        if is_competitor:
            competitor_phrases.append(p)
        else:
            regular_phrases.append(p)
    
    if mode == 'context':
        similarity_threshold = 0.05
        target_clusters_ratio = 25
    else:
        similarity_threshold = 0.08
        target_clusters_ratio = 15
    
    phrases = regular_phrases
    
    if len(phrases) < 5:
        return [{
            'cluster_name': '–í—Å–µ –∑–∞–ø—Ä–æ—Å—ã',
            'total_count': sum(p['count'] for p in phrases),
            'phrases_count': len(phrases),
            'avg_words': round(sum(len(p['phrase'].split()) for p in phrases) / len(phrases), 1),
            'max_frequency': max(p['count'] for p in phrases),
            'min_frequency': min(p['count'] for p in phrases),
            'intent': detect_intent(phrases[0]['phrase']),
            'phrases': sorted(phrases, key=lambda x: x['count'], reverse=True)
        }]
    
    lemmatized = [lemmatize_phrase(p['phrase']) for p in phrases]
    
    tfidf_vectors = calculate_tfidf(lemmatized)
    
    n = len(phrases)
    similarity_matrix = [[0.0] * n for _ in range(n)]
    
    for i in range(n):
        for j in range(i+1, n):
            sim = cosine_similarity_simple(tfidf_vectors[i], tfidf_vectors[j])
            similarity_matrix[i][j] = sim
            similarity_matrix[j][i] = sim
    
    clusters = [[i] for i in range(n)]
    
    target_clusters = max(3, min(20, n // target_clusters_ratio))
    
    while len(clusters) > target_clusters:
        max_sim = -1
        merge_pair = None
        
        for i in range(len(clusters)):
            for j in range(i+1, len(clusters)):
                avg_sim = 0
                count = 0
                for idx_i in clusters[i]:
                    for idx_j in clusters[j]:
                        avg_sim += similarity_matrix[idx_i][idx_j]
                        count += 1
                
                if count > 0:
                    avg_sim /= count
                    if avg_sim > max_sim:
                        max_sim = avg_sim
                        merge_pair = (i, j)
        
        if max_sim < similarity_threshold or merge_pair is None:
            break
        
        i, j = merge_pair
        clusters[i].extend(clusters[j])
        del clusters[j]
    
    stop_words_for_naming = {
        '–∫—É–ø–∏—Ç—å', '–∑–∞–∫–∞–∑–∞—Ç—å', '—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–Ω–µ–¥–æ—Ä–æ–≥–æ', 
        '–¥–µ—à–µ–≤–æ', '–º–æ—Å–∫–≤–∞', '—Å–ø–±', '—Ä–æ—Å—Å–∏—è', '–¥–æ—Å—Ç–∞–≤–∫–∞'
    }
    
    result = []
    for cluster_indices in clusters:
        cluster_phrases = [phrases[i] for i in cluster_indices]
        sorted_phrases = sorted(cluster_phrases, key=lambda x: x['count'], reverse=True)
        
        lemmas_counter = defaultdict(int)
        for idx in cluster_indices:
            lemma = lemmatized[idx]
            for word in lemma.split():
                if len(word) > 2 and word not in stop_words_for_naming:
                    lemmas_counter[word] += 1
        
        if lemmas_counter:
            sorted_words = sorted(lemmas_counter.items(), key=lambda x: x[1], reverse=True)
            top_words = [w for w, _ in sorted_words[:2]]
            cluster_name = ' '.join(top_words).title()
        else:
            words = sorted_phrases[0]['phrase'].split()
            cluster_name = ' '.join(words[:2]).title()
        
        intents = [detect_intent(p['phrase']) for p in cluster_phrases]
        intent_counts = defaultdict(int)
        for intent in intents:
            intent_counts[intent] += 1
        dominant_intent = max(intent_counts.items(), key=lambda x: x[1])[0]
        
        result.append({
            'cluster_name': cluster_name,
            'total_count': sum(p['count'] for p in cluster_phrases),
            'phrases_count': len(cluster_phrases),
            'avg_words': round(sum(len(p['phrase'].split()) for p in cluster_phrases) / len(cluster_phrases), 1),
            'max_frequency': max(p['count'] for p in cluster_phrases),
            'min_frequency': min(p['count'] for p in cluster_phrases),
            'intent': dominant_intent,
            'phrases': sorted_phrases
        })
    
    result.sort(key=lambda x: x['total_count'], reverse=True)
    
    if competitor_phrases:
        result.append({
            'cluster_name': '–ê–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã –∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã',
            'total_count': sum(p['count'] for p in competitor_phrases),
            'phrases_count': len(competitor_phrases),
            'avg_words': round(sum(len(p['phrase'].split()) for p in competitor_phrases) / len(competitor_phrases), 1),
            'max_frequency': max(p['count'] for p in competitor_phrases),
            'min_frequency': min(p['count'] for p in competitor_phrases),
            'intent': 'general',
            'phrases': sorted(competitor_phrases, key=lambda x: x['count'], reverse=True)
        })
    
    return result

def generate_geo_keywords(address: str, base_query: str) -> List[str]:
    '''
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥–µ–æ–∑–∞–≤–∏—Å–∏–º—ã—Ö –≤–∞—Ä–∏–∞—Ü–∏–π –∞–¥—Ä–µ—Å–∞ —á–µ—Ä–µ–∑ OpenAI
    address: "–°—Ç–∞–≤—Ä–æ–ø–æ–ª—å, –ö—É–ª–∞–∫–æ–≤–∞ 1"
    base_query: "–∫—É–ø–∏—Ç—å –∫–≤–∞—Ä—Ç–∏—Ä—É"
    Returns: ["–∫—É–ø–∏—Ç—å –∫–≤–∞—Ä—Ç–∏—Ä—É –ö—É–ª–∞–∫–æ–≤–∞", "–∫—É–ø–∏—Ç—å –∫–≤–∞—Ä—Ç–∏—Ä—É –ö—É–ª–∞–∫–æ–≤–∞ 1", ...]
    '''
    openai_key = os.environ.get('OPENAI_API_KEY')
    proxy_url = os.environ.get('OPENAI_PROXY_URL')
    
    if not openai_key:
        print('[GEO] OpenAI key not found')
        return []
    
    prompt = f"""–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ –∏ –≥–µ–æ–ª–æ–∫–∞—Ü–∏–∏. –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –í–°–ï –≤–æ–∑–º–æ–∂–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø–æ–∏—Å–∫–æ–≤—ã—Ö —Ñ—Ä–∞–∑ –¥–ª—è —ç—Ç–æ–≥–æ –∞–¥—Ä–µ—Å–∞:

–ê–¥—Ä–µ—Å: {address}
–ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å: {base_query}

–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π 15-25 –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ª—é–¥–∏ –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–∏ –ø–æ–∏—Å–∫–µ:
1. –ü–æ–ª–Ω—ã–π –∞–¥—Ä–µ—Å: "–≥–æ—Ä–æ–¥ —É–ª–∏—Ü–∞ –¥–æ–º"
2. –£–ª–∏—Ü–∞ —Å –¥–æ–º–æ–º: "—É–ª–∏—Ü–∞ –¥–æ–º"  
3. –¢–æ–ª—å–∫–æ —É–ª–∏—Ü–∞: "—É–ª–∏—Ü–∞"
4. –†–∞–π–æ–Ω –≥–æ—Ä–æ–¥–∞
5. –û—Ä–∏–µ–Ω—Ç–∏—Ä—ã —Ä—è–¥–æ–º: "—Ä—è–¥–æ–º —Å [–º–µ—Å—Ç–æ]"
6. –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç–Ω—ã–µ —É–∑–ª—ã: "—É –º–µ—Ç—Ä–æ [–Ω–∞–∑–≤–∞–Ω–∏–µ]"  
7. –ú–∏–∫—Ä–æ—Ä–∞–π–æ–Ω—ã
8. –†–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: –¢–û–õ–¨–ö–û —Å–ø–∏—Å–æ–∫ —Ñ—Ä–∞–∑ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é, –∫–∞–∂–¥–∞—è —Ñ—Ä–∞–∑–∞ –¥–æ–ª–∂–Ω–∞ –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å –±–∞–∑–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.
–ü—Ä–∏–º–µ—Ä: –∫—É–ø–∏—Ç—å –∫–≤–∞—Ä—Ç–∏—Ä—É –ö—É–ª–∞–∫–æ–≤–∞, –∫—É–ø–∏—Ç—å –∫–≤–∞—Ä—Ç–∏—Ä—É –ö—É–ª–∞–∫–æ–≤–∞ 1, –∫—É–ø–∏—Ç—å –∫–≤–∞—Ä—Ç–∏—Ä—É –°–µ–≤–µ—Ä–æ-–ó–∞–ø–∞–¥–Ω—ã–π —Ä–∞–π–æ–Ω, –∫—É–ø–∏—Ç—å –∫–≤–∞—Ä—Ç–∏—Ä—É —Ä—è–¥–æ–º —Å –¢—É—Ö–∞—á–µ–≤—Å–∫–∏–º —Ä—ã–Ω–∫–æ–º

–û—Ç–≤–µ—Ç:"""

    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {openai_key}'
    }
    
    payload = {
        'model': 'gpt-4o-mini',
        'messages': [{'role': 'user', 'content': prompt}],
        'temperature': 0.7,
        'max_tokens': 400
    }
    
    proxies = None
    if proxy_url:
        proxies = {'http': proxy_url, 'https': proxy_url}
    
    try:
        response = requests.post(
            'https://api.openai.com/v1/chat/completions',
            headers=headers,
            json=payload,
            proxies=proxies,
            timeout=30
        )
        
        if response.status_code != 200:
            print(f'[GEO] OpenAI error: {response.status_code}')
            return []
        
        result = response.json()
        content = result['choices'][0]['message']['content'].strip()
        
        # Parse comma-separated list
        variations = [v.strip() for v in content.split(',') if v.strip()]
        
        print(f'[GEO] Generated {len(variations)} geo variations')
        return variations[:25]  # Limit to 25
        
    except Exception as e:
        print(f'[GEO] Error: {e}')
        return []

def handler(event: Dict[str, Any], context: Any) -> Dict[str, Any]:
    '''
    Business: –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –Ø–Ω–¥–µ–∫—Å.Wordstat API —Å –°–£–ü–ï–† —É–º–Ω–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–µ–π
    Args: event - dict —Å httpMethod, body (keywords: List[str], regions: List[int])
          context - –æ–±—ä–µ–∫—Ç —Å –∞—Ç—Ä–∏–±—É—Ç–∞–º–∏ request_id, function_name
    Returns: HTTP response —Å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –æ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
    '''
    method: str = event.get('httpMethod', 'GET')
    
    if method == 'OPTIONS':
        return {
            'statusCode': 200,
            'headers': {
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
                'Access-Control-Allow-Headers': 'Content-Type, X-User-Id',
                'Access-Control-Max-Age': '86400'
            },
            'body': ''
        }
    
    headers_dict = event.get('headers', {})
    user_id = headers_dict.get('x-user-id') or headers_dict.get('X-User-Id')
    
    if not user_id:
        return {
            'statusCode': 401,
            'headers': {'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*'},
            'body': json.dumps({'error': 'User ID required'}),
            'isBase64Encoded': False
        }
    
    if not check_subscription(user_id):
        return {
            'statusCode': 403,
            'headers': {'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*'},
            'body': json.dumps({'error': 'Subscription required', 'code': 'SUBSCRIPTION_REQUIRED'}),
            'isBase64Encoded': False
        }
    
    token = os.environ.get('YANDEX_WORDSTAT_TOKEN')
    if not token:
        return {
            'statusCode': 500,
            'headers': {
                'Content-Type': 'application/json',
                'Access-Control-Allow-Origin': '*'
            },
            'isBase64Encoded': False,
            'body': json.dumps({'error': '–¢–æ–∫–µ–Ω –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω. –î–æ–±–∞–≤—å—Ç–µ YANDEX_WORDSTAT_TOKEN.'})
        }
    
    if method == 'GET':
        try:
            api_url = 'https://api.wordstat.yandex.net/v1/getRegionsTree'
            headers = {
                'Authorization': f'Bearer {token}',
                'Content-Type': 'application/json; charset=utf-8',
                'Accept-Language': 'ru'
            }
            
            response = requests.get(api_url, headers=headers, timeout=30)
            
            if response.status_code != 200:
                return {
                    'statusCode': response.status_code,
                    'headers': {
                        'Content-Type': 'application/json',
                        'Access-Control-Allow-Origin': '*'
                    },
                    'isBase64Encoded': False,
                    'body': json.dumps({'error': f'API error: {response.status_code}'})
                }
            
            data = response.json()
            
            return {
                'statusCode': 200,
                'headers': {
                    'Content-Type': 'application/json',
                    'Access-Control-Allow-Origin': '*'
                },
                'isBase64Encoded': False,
                'body': json.dumps({'success': True, 'regions': data})
            }
        except Exception as e:
            print(f'[REGIONS ERROR] {str(e)}')
            return {
                'statusCode': 500,
                'headers': {
                    'Content-Type': 'application/json',
                    'Access-Control-Allow-Origin': '*'
                },
                'isBase64Encoded': False,
                'body': json.dumps({'error': str(e)})
            }
    
    if method == 'POST':
        body_str = event.get('body', '{}')
        if not body_str or body_str.strip() == '':
            body_str = '{}'
        body_data = json.loads(body_str)
        keywords: List[str] = body_data.get('keywords', [])
        regions: List[int] = body_data.get('regions', [213])
        use_openai: bool = body_data.get('use_openai', True)
        object_address: str = body_data.get('objectAddress', '')
        region_names: List[str] = body_data.get('region_names', [])
        selected_intents: List[str] = body_data.get('selected_intents', [])
        
        print(f'[WORDSTAT] Request params: keywords={keywords}, regions={regions}, use_openai={use_openai}')
        print(f'[WORDSTAT] Body data: {body_data}')
        
        if not keywords or len(keywords) == 0 or not keywords[0].strip():
            return {
                'statusCode': 400,
                'headers': {
                    'Content-Type': 'application/json',
                    'Access-Control-Allow-Origin': '*'
                },
                'isBase64Encoded': False,
                'body': json.dumps({'error': '–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞'})
            }
        
        api_url = 'https://api.wordstat.yandex.net/v1/topRequests'
        headers = {
            'Authorization': f'Bearer {token}',
            'Content-Type': 'application/json; charset=utf-8',
            'Accept-Language': 'ru'
        }
        
        try:
            payload = {
                'phrase': keywords[0],
                'regions': regions
            }
            
            print(f'[WORDSTAT] Request payload: phrase={keywords[0]}, regions={regions}')
            
            response = requests.post(api_url, json=payload, headers=headers, timeout=30)
            
            if response.status_code != 200:
                return {
                    'statusCode': response.status_code,
                    'headers': {
                        'Content-Type': 'application/json',
                        'Access-Control-Allow-Origin': '*'
                    },
                    'isBase64Encoded': False,
                    'body': json.dumps({'error': f'API error: {response.status_code}'})
                }
            
            data = response.json()
            
            if 'error' in data:
                return {
                    'statusCode': 400,
                    'headers': {
                        'Content-Type': 'application/json',
                        'Access-Control-Allow-Origin': '*'
                    },
                    'isBase64Encoded': False,
                    'body': json.dumps({'error': data.get('error')})
                }
            
            top_requests = data.get('topRequests', [])
            clustering_mode = body_data.get('mode', 'seo')
            print(f'[WORDSTAT] Got {len(top_requests)} phrases from Yandex API, mode: {clustering_mode}')
            
            if use_openai:
                print(f'[WORDSTAT] Using advanced TF-IDF clustering')
                clusters, minus_words = clusterize_advanced(
                    top_requests, 
                    mode=clustering_mode,
                    region_names=region_names,
                    selected_intents=selected_intents
                )
            else:
                print('[WORDSTAT] Using TF-IDF for clustering')
                clusters = smart_clusterize(top_requests, mode=clustering_mode)
                minus_words = {}
                if clustering_mode == 'context':
                    minus_words = detect_minus_words(top_requests)
            
            print(f'[WORDSTAT] Created {len(clusters)} smart clusters ({clustering_mode} mode)')
            if minus_words:
                total_minus = sum(v.get('count', 0) if isinstance(v, dict) else len(v) if isinstance(v, list) else 0 for v in minus_words.values())
                print(f'[WORDSTAT] Detected {total_minus} minus-words')
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≥–µ–æ–∫–ª—é—á–∏, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –∞–¥—Ä–µ—Å –æ–±—ä–µ–∫—Ç–∞
            geo_cluster = None
            if object_address and object_address.strip():
                print(f'[GEO] Generating geo keywords for: {object_address}')
                geo_keywords = generate_geo_keywords(object_address, keywords[0])
                
                if geo_keywords:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å –≤ Wordstat
                    geo_phrases = []
                    for geo_kw in geo_keywords[:15]:  # Limit to 15 requests
                        try:
                            payload_geo = {'phrase': geo_kw, 'regions': regions}
                            resp_geo = requests.post(api_url, json=payload_geo, headers=headers, timeout=10)
                            if resp_geo.status_code == 200:
                                data_geo = resp_geo.json()
                                top_req_geo = data_geo.get('topRequests', [])
                                if top_req_geo and top_req_geo[0]['count'] > 10:  # Min frequency 10
                                    geo_phrases.append(top_req_geo[0])
                        except:
                            pass
                    
                    if geo_phrases:
                        geo_cluster = {
                            'cluster_name': 'üìç –ì–µ–æ–ª–æ–∫–∞—Ü–∏—è',
                            'total_count': sum(p['count'] for p in geo_phrases),
                            'phrases_count': len(geo_phrases),
                            'avg_words': round(sum(len(p['phrase'].split()) for p in geo_phrases) / len(geo_phrases), 1),
                            'max_frequency': max(p['count'] for p in geo_phrases),
                            'min_frequency': min(p['count'] for p in geo_phrases),
                            'intent': 'commercial',
                            'phrases': sorted(geo_phrases, key=lambda x: x['count'], reverse=True)
                        }
                        clusters.insert(0, geo_cluster)  # Add as first cluster
                        print(f'[GEO] Added geo cluster with {len(geo_phrases)} phrases')
            
            search_query = [{
                'Keyword': keywords[0],
                'Shows': top_requests[0]['count'] if top_requests else 0,
                'TopRequests': top_requests,
                'Clusters': clusters,
                'MinusWords': minus_words,
                'Mode': clustering_mode,
                'GeoCluster': geo_cluster
            }]
        
        except requests.exceptions.Timeout:
            return {
                'statusCode': 504,
                'headers': {
                    'Content-Type': 'application/json',
                    'Access-Control-Allow-Origin': '*'
                },
                'isBase64Encoded': False,
                'body': json.dumps({'error': '–ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç API'})
            }
        except Exception as e:
            print(f'[WORDSTAT ERROR] {str(e)}')
            import traceback
            traceback.print_exc()
            return {
                'statusCode': 500,
                'headers': {
                    'Content-Type': 'application/json',
                    'Access-Control-Allow-Origin': '*'
                },
                'isBase64Encoded': False,
                'body': json.dumps({'error': f'–û—à–∏–±–∫–∞: {str(e)}'})
            }
        
        return {
            'statusCode': 200,
            'headers': {
                'Content-Type': 'application/json',
                'Access-Control-Allow-Origin': '*'
            },
            'isBase64Encoded': False,
            'body': json.dumps({
                'success': True,
                'data': {
                    'SearchQuery': search_query
                }
            }, ensure_ascii=False)
        }
    
    return {
        'statusCode': 405,
        'headers': {
            'Content-Type': 'application/json',
            'Access-Control-Allow-Origin': '*'
        },
        'isBase64Encoded': False,
        'body': json.dumps({'error': 'Method not allowed'})
    }